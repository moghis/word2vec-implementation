{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21065,
     "status": "ok",
     "timestamp": 1639953442280,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "oyK7gL2KwbRu",
    "outputId": "a2016145-be99-4009-dbab-88322670cd08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "(100002346,)\n",
      "Word Count is: 12125\n",
      "Word Count Sum is 55596\n",
      "Sentence Count is: 4537\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "fulton county grand jury friday investigation of atlantas recent primary election produced no evidence that any irregularities took place\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "jury further termend presentments city executive committee had overall charge election deserves praise thanks city atlanta manner in which election conducted\n",
      "positive:\n",
      "[([12125, 12125, 2, 3], 1, [3484, 4844, 1537, 2122, 18, 75, 1915, 5591]), ([12125, 1, 3, 4], 2, [666, 2720, 204, 10988, 3125, 11522, 3763, 10134]), ([1, 2, 4, 6], 3, [11882, 3876, 10570, 5742, 0, 5893, 68, 3527]), ([2, 3, 6, 8], 4, [7932, 640, 332, 373, 9289, 338, 317, 3627]), ([3, 4, 8, 9], 6, [1902, 843, 9, 2395, 1973, 437, 1094, 1665]), ([4, 6, 9, 10], 8, [303, 6185, 1287, 4489, 10821, 752, 52, 1878]), ([6, 8, 10, 11], 9, [9717, 1157, 1862, 4588, 1773, 7120, 16, 3672]), ([8, 9, 11, 12], 10, [8958, 2100, 515, 11877, 1278, 1601, 11263, 11700]), ([9, 10, 12, 13], 11, [9373, 5782, 2502, 2025, 1190, 1754, 10172, 11609]), ([10, 11, 13, 14], 12, [8063, 2860, 3162, 7241, 1237, 1570, 7973, 735])]\n",
      "[(['county', 'grand'], 'fulton', ['alexander', 'minoso', 'expand', 'looked', 'any', 'it', 'realization', 'theyre']), (['fulton', 'grand', 'jury'], 'county', ['like', 'organizations', 'we', 'teeth', 'ninetynine', 'belgians', 'firemen', 'jussel']), (['fulton', 'county', 'jury', 'friday'], 'grand', ['morethanordinary', 'her', 'busywork', 'embassies', 'the', 'bulletind', 'considering', 'cooperate']), (['county', 'grand', 'friday', 'investigation'], 'jury', ['jewelry', 'pay', 'couple', 'interlude', 'cookie', 'mrs', 'health', 'schedule']), (['grand', 'jury', 'investigation', 'of'], 'friday', ['union', 'because', 'of', 'expert', 'interested', 'out', 'kind', 'direct']), (['jury', 'friday', 'of', 'atlantas'], 'investigation', ['plan', 'informally', 'york', 'knee', 'mourn', 'local', 'to', 'application']), (['friday', 'investigation', 'atlantas', 'recent'], 'of', ['library', 'providing', 'affair', 'stayed', 'allies', 'otis', 'evidence', 'parks']), (['investigation', 'of', 'recent', 'primary'], 'atlantas', ['clothes', 'kremlin', 'where', 'poors', 'discrimination', 'requirements', 'stances', 'eject']), (['of', 'atlantas', 'primary', 'election'], 'recent', ['theyd', 'mollys', 'consent', 'tendency', 'c', 'climate', 'listening', 'kasavubu']), (['atlantas', 'recent', 'election', 'produced'], 'primary', ['anniston', 'she', 'britain', 'bottled', 'holds', 'nine', 'audiovisual', 'george'])]\n",
      "10\n",
      "positive:\n",
      "[([11, 12, 14, 15], 13, [2587, 9299, 35, 10823, 97, 7494, 2728, 46]), ([12, 13, 15, 16], 14, [204, 164, 2350, 8377, 592, 120, 665, 185]), ([13, 14, 16, 17], 15, [7788, 75, 8510, 82, 413, 4003, 63, 11914]), ([14, 15, 17, 18], 16, [8851, 258, 1406, 2817, 6781, 52, 8930, 23]), ([15, 16, 18, 19], 17, [4897, 6518, 5130, 9212, 2038, 2460, 30, 150]), ([16, 17, 19, 20], 18, [5747, 854, 683, 8309, 564, 5, 953, 2446]), ([17, 18, 20, 21], 19, [0, 4489, 8412, 2653, 2941, 3474, 383, 5237]), ([18, 19, 21, 12125], 20, [1155, 1182, 9434, 300, 6230, 5843, 7382, 6398]), ([19, 20, 12125, 12125], 21, [5027, 2587, 1445, 748, 4073, 6378, 3119, 0]), ([12125, 12125, 22, 24], 4, [9495, 4983, 1620, 11936, 268, 387, 1392, 6810])]\n",
      "[(['recent', 'primary', 'produced', 'no'], 'election', ['open', 'clara', 'and', 'parading', 'improving', 'operators', 'shown', 'by']), (['primary', 'election', 'no', 'evidence'], 'produced', ['we', 'at', 'across', 'allotting', 'about', 'two', 'washington', 'through']), (['election', 'produced', 'evidence', 'that'], 'no', ['barbecue', 'it', 'makers', 'are', 'enthusiastic', 'opening', 'a', 'fiat']), (['produced', 'no', 'that', 'any'], 'evidence', ['suspects', 'new', 'much', 'tracts', 'pullen', 'to', 'sarmi', 'in']), (['no', 'evidence', 'any', 'irregularities'], 'that', ['zoe', 'arrange', 'ceremony', 'resist', 'cuba', 'explained', 'had', 'outgoing']), (['evidence', 'that', 'irregularities', 'took'], 'any', ['activities', 'opposition', 'hot', 'steins', 'most', 'said', 'tell', 'presented']), (['that', 'any', 'took', 'place'], 'irregularities', ['the', 'knee', 'chip', 'ahead', 'definite', 'tried', 'who', 'missouri']), (['any', 'irregularities', 'place'], 'took', ['felt', 'poor', 'moultons', 'fair', 'restaurants', 'phoenix', 'businessman', 'reputation']), (['irregularities', 'took'], 'place', ['tv', 'open', 'demands', 'too', 'dropped', 'lewelleyn', 'completed', 'the']), (['further', 'termend'], 'jury', ['sufficiently', 'reaches', 'nations', 'textile', 'not', 'after', 'white', 'operations'])]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences, sample):\n",
    "        self.norm_sentences = []\n",
    "        self.counter = 0\n",
    "        self.sample = sample\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  #  Number of words\n",
    "        self.word_count_sum = 0  # Total number of words\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict(sentences)  # Initialize the dictionary\n",
    "        self.subsampling()\n",
    "        self.sample_table = []\n",
    "        self._init_sample_table()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "\n",
    "\n",
    "    def subsampling(self):\n",
    "        \n",
    "        if self.sample > 0:\n",
    "            self.word_count_sum = 0\n",
    "            self.sentence_count = 0\n",
    "\n",
    "            frequency = np.array(list(self.wordId_frequency_dict.values()))\n",
    "            z = frequency / np.sum(frequency)\n",
    "            p = (np.sqrt(z / self.sample) + 1) * (self.sample / z)\n",
    "\n",
    "            new_norm_sentences = []\n",
    "            for word_list in self.norm_sentences:\n",
    "              word_list = [word for word in word_list if p[self.word2id_dict[word]] > random.random()]\n",
    "              if len(word_list) >= 2:\n",
    "                self.sentence_count += 1\n",
    "                self.word_count_sum += len(word_list)\n",
    "                new_norm_sentences.append(word_list)\n",
    "\n",
    "            self.norm_sentences = new_norm_sentences\n",
    "\n",
    "    def normalize(self, word_list):\n",
    "      sentence = \" \".join(word for word in word_list)\n",
    "      sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "      sentence = sentence.lower()\n",
    "      sentence = re.sub(' +', ' ', sentence)\n",
    "      sentence = sentence.strip()\n",
    "      norm_word_list = sentence.split(' ')\n",
    "      if self.sample <= 0:\n",
    "          stop_words = nltk.corpus.stopwords.words('english')\n",
    "          norm_word_list_with_out_stop_words = [word for word in norm_word_list if word not in stop_words]\n",
    "          norm_word_list = norm_word_list_with_out_stop_words\n",
    "\n",
    "      return norm_word_list\n",
    "\n",
    "    def _init_dict(self,sentences):\n",
    "        word_freq = dict()\n",
    "        for word_list in sentences:\n",
    "            word_list = self.normalize(word_list)\n",
    "            if(len(word_list) < 2):\n",
    "                continue\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "            self.norm_sentences.append(word_list)\n",
    "        word_id = 0\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def _init_sample_table(self):\n",
    "        sample_table_size = 1e8\n",
    "        frequency = np.array(list(self.wordId_frequency_dict.values())) ** 0.75\n",
    "        frequency_sum = sum(frequency)\n",
    "        ratio_array = frequency / frequency_sum \n",
    "        word_count_list = np.round(ratio_array * sample_table_size)\n",
    "        for word_index, word_freq in enumerate(word_count_list):\n",
    "            self.sample_table += [word_index] * int(word_freq)  # it generates a list, the content is the id of each word, each id in the list is repeated multiple times, the number of repetitions is the word frequency\n",
    "        self.sample_table = np.array(self.sample_table)\n",
    "        print(self.sample_table.shape)\n",
    "\n",
    "    def generate_positive_pairs(self, window_size, neg_count):\n",
    "        self.counter += 1\n",
    "        if not self.norm_sentences[20*(self.counter-1):20*self.counter]:\n",
    "            self.counter = 1\n",
    "            self.word_pairs_queue.clear()\n",
    "        sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.norm_sentences[20*(self.counter-1):20*self.counter]]\n",
    "\n",
    "        for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, center_word in enumerate(words):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words = []\n",
    "            for index_2 in range(start,end):\n",
    "              if 0 <= index_2 < sentence_length and index_2 != index:\n",
    "                context_words.append(words[index_2])\n",
    "              elif index_2 < 0 or index_2 >= sentence_length:\n",
    "                context_words.append(self.word_count)\n",
    "            \n",
    "            negative_words = np.random.choice(self.sample_table, size=neg_count).tolist()\n",
    "\n",
    "            self.word_pairs_queue.append((context_words, center_word, negative_words))\n",
    "           \n",
    "\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size, neg_count):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_positive_pairs(window_size, neg_count)              \n",
    "              \n",
    "        result_pairs = []\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])\n",
    "    test_data = InputData(sentences,0.0002)\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[1]))\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append(([test_data.id2word_dict[i] for i in pair[0] if i != test_data.word_count], test_data.id2word_dict[pair[1]], [test_data.id2word_dict[i] for i in pair[2]]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append(([test_data.id2word_dict[i] for i in pair[0] if i != test_data.word_count], test_data.id2word_dict[pair[1]], [test_data.id2word_dict[i] for i in pair[2]]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7317,
     "status": "ok",
     "timestamp": 1639953515210,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "UlIY61qyQ8QW",
    "outputId": "87907694-673b-403a-a10b-63a1c1e11ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(self.emb_size + 1, self.emb_dimension, sparse=True)  # Define the embedded dictionary for the input word\n",
    "        self.w_embeddings = nn.Embedding(self.emb_size, self.emb_dimension, sparse=True)  # Define the embedded dictionary for the output word\n",
    "        self._init_embedding()  # initialization\n",
    "\n",
    "    def _init_embedding(self):\n",
    "        int_range = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-int_range, int_range)\n",
    "        self.w_embeddings.weight.data.uniform_(-0, 0) \n",
    "\n",
    "    def compute_context_matrix(self, u):\n",
    "        pos_u_emb = self.u_embeddings(torch.LongTensor(u))\n",
    "        pos_u_emb = torch.mean(pos_u_emb, 1, True)\n",
    "        pos_u_emb = pos_u_emb.squeeze()\n",
    "\n",
    "        return pos_u_emb\n",
    "\n",
    "    def forward(self, pos_u, pos_w, neg_w):\n",
    "        pos_u_emb = self.compute_context_matrix(pos_u)\n",
    "        pos_w_emb = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "        neg_w_emb = self.w_embeddings(torch.LongTensor(neg_w))\n",
    "\n",
    "        score = torch.mul(pos_u_emb, pos_w_emb)\n",
    "        score = torch.sum(score, dim=1).squeeze()\n",
    "        score = F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.mul(neg_w_emb, pos_u_emb.unsqueeze(1))\n",
    "        neg_score = torch.sum(neg_score, dim=2).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        neg_score = torch.sum(neg_score, dim=1).squeeze()\n",
    "\n",
    "        final_score = score + neg_score\n",
    "        loss = -1 * torch.sum(final_score)\n",
    "        return loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.u_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = CBOWModel(100, 2)\n",
    "\n",
    "    pos_u = [[9, 1],[0, 1]]\n",
    "    pos_w = [2, 4]\n",
    "    neg_w = [[9, 1, 7, 3],[1, 7, 6, 8]]\n",
    "    model.forward(pos_u, pos_w, neg_w)\n",
    "    distance_matrix = model.distance_matrix(5)\n",
    "    print(distance_matrix.shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1639953521250,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "Zaen9N8EYD8y"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 2 \n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.05 # Learning rate\n",
    "NEG_COUNT = 8\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, sentences, sample):\n",
    "        self.data = InputData(sentences, sample)\n",
    "        self.model = CBOWModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        lambda1 = lambda epoch: 0.99 ** epoch\n",
    "        self.scheduler = LambdaLR(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"SkipGram Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,51):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE, NEG_COUNT)\n",
    "                pos_u = [pair[0] for pair in pairs]\n",
    "                pos_w = [int(pair[1]) for pair in pairs]\n",
    "                neg_w = [pair[2] for pair in pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u, pos_w, neg_w)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12455,
     "status": "ok",
     "timestamp": 1639953553657,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "UNUCoHoDZQWn",
    "outputId": "1fe13515-d4cf-434e-e480-90004d139b26",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100002426,)\n",
      "Word Count is: 24758\n",
      "Word Count Sum is 196848\n",
      "Sentence Count is: 17277\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "SAMPLE = 0.0002 # use subsampling\n",
    "w2v = Word2Vec(sentences, SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 650855,
     "status": "ok",
     "timestamp": 1639954236543,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "g-71OOLAZSDZ",
    "outputId": "71bb12fb-37c0-4c9a-dc10-69a6cc7181a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Training......\n",
      "pairs_count 196848\n",
      "batch_count 196.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(4478.0581, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(3574.5586, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(3423.7976, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(3421.0486, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(3394.6873, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(3310.4585, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(3312.1855, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(3237.0649, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(3174.7947, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(3157.9053, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(3007.0750, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(2982.8210, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(2912.1064, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(2839.3059, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(2759.7563, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(2634.8887, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(2619.8528, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(2465.1301, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(2438.0911, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(2340.3450, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 loss: tensor(2208.0608, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 loss: tensor(2103.8403, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 loss: tensor(2045.8989, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 loss: tensor(1979.8619, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 loss: tensor(1876.6068, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 loss: tensor(1838.4877, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 loss: tensor(1796.0032, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 loss: tensor(1651.6262, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: tensor(1626.6632, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: tensor(1600.1187, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 loss: tensor(1517.2380, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss: tensor(1480.1741, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 loss: tensor(1412.3494, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 loss: tensor(1368.3174, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 loss: tensor(1292.1074, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 loss: tensor(1251.9280, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 loss: tensor(1249.6040, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 loss: tensor(1187.7280, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 loss: tensor(1144.9365, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: tensor(1119.3293, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:14<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 loss: tensor(1073.5837, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 loss: tensor(1057.5358, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 loss: tensor(1010.0784, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 loss: tensor(983.0143, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 loss: tensor(903.2000, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 loss: tensor(988.5723, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 loss: tensor(904.3854, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 loss: tensor(883.0977, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss: tensor(861.6632, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:13<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(814.1398, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 17259,
     "status": "ok",
     "timestamp": 1639954842998,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "H_JPRgCJZUyT"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1639955254192,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "71MoirSjZYvf",
    "outputId": "c1fe2735-62ee-4999-8477-0914425c5a53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bockwurst': ['knackwurst',\n",
       "  'benzedrine',\n",
       "  'bacillus',\n",
       "  'zhitkov',\n",
       "  'apergillus',\n",
       "  'bologna',\n",
       "  'selfdeceptions',\n",
       "  'orzae',\n",
       "  'kob'],\n",
       " 'football': ['basketball',\n",
       "  'cincinnati',\n",
       "  'dakota',\n",
       "  'ron',\n",
       "  'milwaukee',\n",
       "  'ave',\n",
       "  'homer',\n",
       "  'prince',\n",
       "  'arkansas'],\n",
       " 'republican': ['nomination',\n",
       "  'fbi',\n",
       "  'gubernatorial',\n",
       "  'casey',\n",
       "  'van',\n",
       "  'allen',\n",
       "  'rep',\n",
       "  'howard',\n",
       "  'barnard'],\n",
       " 'sauce': ['tablespoons',\n",
       "  'tablespoon',\n",
       "  'chili',\n",
       "  'creekturn',\n",
       "  'minced',\n",
       "  'teaspoon',\n",
       "  'mustard',\n",
       "  'cloth',\n",
       "  'bread'],\n",
       " 'tablespoon': ['teaspoon',\n",
       "  'worcestershire',\n",
       "  'chili',\n",
       "  'pickled',\n",
       "  'dia',\n",
       "  'teaspoons',\n",
       "  'toner',\n",
       "  'vinegar',\n",
       "  'canned'],\n",
       " 'university': ['emory',\n",
       "  'commissioner',\n",
       "  'queen',\n",
       "  'mathematics',\n",
       "  'universitys',\n",
       "  'showmanship',\n",
       "  'houston',\n",
       "  'republicans',\n",
       "  'formerly']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','sauce', 'republican','football','bockwurst','university']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNL1MHuu4BNDCoqGjTfxB2k",
   "collapsed_sections": [],
   "name": "Word2Vec - CBOW with negative sampling and subsampling.ipynb",
   "provenance": [
    {
     "file_id": "1nqJwWIWBqPOLloLD_q9W5CHnUwIgTCfX",
     "timestamp": 1639936170325
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
