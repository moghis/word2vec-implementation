{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11181,
     "status": "ok",
     "timestamp": 1639852722645,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "TKbGaf5uYpaD",
    "outputId": "dd84915d-ecc9-4625-90b1-e2d97169a519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(100002346,)\n",
      "Word Count is: 12125\n",
      "Word Count Sum is 86971\n",
      "Sentence Count is: 4573\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "fulton county grand jury friday an investigation atlantas recent primary election produced evidence that any irregularities took place\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "jury further termend presentments that city executive committee had overall charge the election deserves praise thanks of atlanta manner which election was conducted\n",
      "positive:\n",
      "[(1, [12125, 12125, 2, 3], [10168, 164, 205, 767, 10142, 612, 40, 1651]), (2, [12125, 1, 3, 4], [4138, 2491, 20, 325, 6551, 2030, 316, 306]), (3, [1, 2, 4, 6], [8917, 564, 3917, 2075, 578, 4862, 35, 10047]), (4, [2, 3, 6, 7], [464, 6960, 17, 3729, 4072, 11695, 3163, 11087]), (6, [3, 4, 7, 8], [2870, 1345, 2309, 10226, 7926, 478, 1402, 905]), (7, [4, 6, 8, 10], [529, 369, 11927, 6640, 17, 4971, 16, 4436]), (8, [6, 7, 10, 11], [3231, 573, 2277, 6245, 842, 3508, 0, 1517]), (10, [7, 8, 11, 12], [208, 23, 63, 2660, 11207, 23, 180, 756]), (11, [8, 10, 12, 13], [2869, 0, 1037, 2170, 52, 3877, 4317, 181]), (12, [10, 11, 13, 14], [4093, 521, 2951, 8009, 10382, 38, 78, 11656])]\n",
      "[('fulton', ['county', 'grand'], ['lemonade', 'at', 'feel', 'approval', 'approached', 'can', 'was', 'unspecified']), ('county', ['fulton', 'grand', 'jury'], ['fashion', 'towns', 'took', 'his', 'chairmen', 'situation', 'hospital', 'operation']), ('grand', ['fulton', 'county', 'jury', 'friday'], ['adjustment', 'most', 'pm', 'efforts', 'old', 'fly', 'and', 'expensive']), ('jury', ['county', 'grand', 'friday', 'an'], ['first', 'canadian', 'that', 'driveway', 'majorleague', 'flickered', 'maurice', 'birdied']), ('friday', ['grand', 'jury', 'an', 'investigation'], ['mates', 'defense', 'level', 'pike', 'eddy', 'ballot', 'american', 'designed']), ('an', ['jury', 'friday', 'investigation', 'atlantas'], ['what', 'henry', 'exports', 'downtown', 'that', 'nehf', 'evidence', 'kerr']), ('investigation', ['friday', 'an', 'atlantas', 'recent'], ['ride', 'fund', 'station', 'shamrock', 'now', 'contract', 'the', 'care']), ('atlantas', ['an', 'investigation', 'recent', 'primary'], ['some', 'in', 'a', 'leaders', 'weigh', 'in', 'but', 'promised']), ('recent', ['investigation', 'atlantas', 'primary', 'election'], ['beame', 'the', 'requirement', 'appear', 'to', 'please', 'sundays', 'has']), ('primary', ['atlantas', 'recent', 'election', 'produced'], ['hyde', 'senate', 'remarks', 'parrillo', 'embassy', 'for', 'many', 'secretarygeneral'])]\n",
      "10\n",
      "positive:\n",
      "[(13, [11, 12, 14, 16], [21, 3193, 1316, 7763, 2123, 410, 4531, 4733]), (14, [12, 13, 16, 17], [149, 9264, 541, 8073, 1151, 132, 3768, 2383]), (16, [13, 14, 17, 18], [4120, 4994, 1673, 35, 3121, 779, 966, 4251]), (17, [14, 16, 18, 19], [137, 186, 5166, 28, 1584, 1013, 985, 0]), (18, [16, 17, 19, 20], [361, 541, 996, 1979, 616, 435, 3521, 783]), (19, [17, 18, 20, 21], [408, 1353, 864, 9279, 10540, 2286, 3225, 564]), (20, [18, 19, 21, 12125], [6639, 0, 10046, 11425, 1796, 776, 739, 63]), (21, [19, 20, 12125, 12125], [316, 695, 8619, 11317, 5571, 2833, 16, 4085]), (4, [12125, 12125, 22, 24], [29, 11188, 783, 2600, 2348, 188, 395, 1020]), (22, [12125, 4, 24, 25], [5627, 3782, 8426, 578, 2099, 63, 139, 35])]\n",
      "[('election', ['recent', 'primary', 'produced', 'evidence'], ['place', 'saturday', 'able', 'dorenzo', 'show', 'blue', 'muscle', 'tough']), ('produced', ['primary', 'election', 'evidence', 'that'], ['also', 'mmes', 'earlier', 'grooms', 'could', 'is', 'guards', 'factory']), ('evidence', ['election', 'produced', 'that', 'any'], ['proved', 'delahanty', 'lines', 'and', 'due', 'led', 'question', 'yearold']), ('that', ['produced', 'evidence', 'any', 'irregularities'], ['as', 'all', 'dance', 'committee', 'visiting', 'without', 'later', 'the']), ('any', ['evidence', 'that', 'irregularities', 'took'], ['more', 'earlier', 'committees', 'attacks', 'given', 'force', 'fired', 'down']), ('irregularities', ['that', 'any', 'took', 'place'], ['held', 'eight', 'if', 'vegas', 'intangibles', 'know', 'hundreds', 'most']), ('took', ['any', 'irregularities', 'place'], ['ballroom', 'the', 'leather', 'braques', 'tension', 'bankers', 'himself', 'a']), ('place', ['irregularities', 'took'], ['hospital', 'davis', 'upturn', 'pentagon', 'successfully', 'sympathetic', 'evidence', 'frames']), ('jury', ['further', 'termend'], ['which', 'airconditioning', 'down', 'unemployment', 'shirking', 'with', 'strong', 'gulf']), ('further', ['jury', 'termend', 'presentments'], ['quietly', 'contested', 'competitive', 'old', 'early', 'a', 'policies', 'and'])]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences):\n",
    "        self.norm_sentences = []\n",
    "        self.counter = 0\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  #  Number of words (repeated words only count as 1)\n",
    "        self.word_count_sum = 0  # Total number of words (the number of repeated words also accumulates)\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict(sentences)  # Initialize the dictionary\n",
    "        self.subsampling()\n",
    "        self.sample_table = []\n",
    "        self._init_sample_table()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "\n",
    "    def special_match(strg, search=re.compile(r'[^a-z0-9.]').search):\n",
    "      return not bool(search(strg))\n",
    "\n",
    "\n",
    "    def subsampling(self):\n",
    "        t = 0.0003\n",
    "        frequency = np.array(list(self.wordId_frequency_dict.values()))\n",
    "        z = frequency / sum(frequency)\n",
    "        p = (np.sqrt(z / t) + 1) * (t / z)\n",
    "\n",
    "        for index, word_list in enumerate(self.norm_sentences):\n",
    "          word_list = [word for word in word_list if p[self.word2id_dict[word]] > random.random()]\n",
    "          self.norm_sentences[index] = word_list\n",
    "\n",
    "    def normalize(self, word_list):\n",
    "      sentence = \" \".join(word for word in word_list)\n",
    "      sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "      sentence = sentence.lower()\n",
    "      sentence = re.sub(' +', ' ', sentence)\n",
    "      sentence = sentence.strip()\n",
    "      norm_word_list = sentence.split(' ')\n",
    "\n",
    "      return norm_word_list\n",
    "       \n",
    "\n",
    "    def _init_dict(self, sentences):\n",
    "        word_freq = dict()\n",
    "        for word_list in sentences:\n",
    "            word_list = self.normalize(word_list)\n",
    "            if(len(word_list) < 2):\n",
    "                continue\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "            self.norm_sentences.append(word_list)\n",
    "\n",
    "        word_id = 0\n",
    "        # Initialize word2id_dict, id2word_dict, wordId_frequency_dict dictionary\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def _init_sample_table(self):\n",
    "        sample_table_size = 1e8\n",
    "        frequency = np.array(list(self.wordId_frequency_dict.values())) ** 0.75\n",
    "        frequency_sum = sum(frequency)  # Total word frequency of all words\n",
    "        ratio_array = frequency / frequency_sum \n",
    "        word_count_list = np.round(ratio_array * sample_table_size)\n",
    "        for word_index, word_freq in enumerate(word_count_list):\n",
    "            self.sample_table += [word_index] * int(word_freq)  # it generates a list, the content is the id of each word, each id in the list is repeated multiple times, the number of repetitions is the word frequency\n",
    "        self.sample_table = np.array(self.sample_table)\n",
    "        print(self.sample_table.shape)\n",
    "\n",
    "    def generate_positive_pairs(self, window_size, neg_count):\n",
    "        self.counter += 1\n",
    "        if not self.norm_sentences[20*(self.counter-1):20*self.counter]:\n",
    "            self.counter = 1\n",
    "            self.word_pairs_queue.clear()\n",
    "        sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.norm_sentences[20*(self.counter-1):20*self.counter]]\n",
    "\n",
    "        # Find the positive sampling pair (w,v) and add it to the positive sampling queue\n",
    "        for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, center_word in enumerate(words):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            positive_words = []\n",
    "            for index_2 in range(start,end):\n",
    "              if 0 <= index_2 < sentence_length and index_2 != index:\n",
    "                positive_words.append(words[index_2])\n",
    "              elif index_2 < 0 or index_2 >= sentence_length:\n",
    "                positive_words.append(self.word_count)\n",
    "            \n",
    "            negative_words = np.random.choice(self.sample_table, size=neg_count).tolist()\n",
    "\n",
    "            self.word_pairs_queue.append((center_word, positive_words, negative_words))\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size, neg_count):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_positive_pairs(window_size, neg_count)              \n",
    "              \n",
    "        result_pairs = []  # Returns a positive sample pair of mini-batch size\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])\n",
    "    test_data = InputData(sentences)\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[1]))\n",
    "    # test_data.evaluate_pairs_count()\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], [test_data.id2word_dict[i] for i in pair[1] if i != test_data.word_count], [test_data.id2word_dict[i] for i in pair[2] if i != test_data.word_count]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], [test_data.id2word_dict[i] for i in pair[1] if i != test_data.word_count], [test_data.id2word_dict[i] for i in pair[2] if i != test_data.word_count]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "    # neg_pair = test_data.get_negative_sampling(pos_pairs, 3)\n",
    "    # print('negative:')\n",
    "    # print(neg_pair)\n",
    "    # neg_word_pair = []\n",
    "    #for pair in neg_pair:\n",
    "    #    neg_word_pair.append(\n",
    "    #        (test_data.id2word_dict[pair[0]], test_data.id2word_dict[pair[1]], test_data.id2word_dict[pair[2]]))\n",
    "    #print(neg_word_pair)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1639850392584,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "UunHC6-rZBpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.w_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size + 1, emb_dimension, sparse=True)\n",
    "        self._init_emb()\n",
    "\n",
    "    def _init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange) # work\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0) # work\n",
    "\n",
    "    def forward(self, pos_w, pos_v, neg_v):\n",
    "        emb_w = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "        emb_v = self.v_embeddings(torch.LongTensor(pos_v))\n",
    "        neg_emb_v = self.v_embeddings(torch.LongTensor(neg_v))\n",
    "\n",
    "        score = torch.mul(emb_w.unsqueeze(1), emb_v)\n",
    "        score = torch.sum(score, dim=2).squeeze()\n",
    "        score = F.logsigmoid(score)\n",
    "        score = torch.sum(score, dim=1).squeeze()\n",
    "\n",
    "        neg_score = torch.mul(emb_w.unsqueeze(1), neg_emb_v)\n",
    "        neg_score = torch.sum(neg_score, dim=2).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        neg_score = torch.sum(neg_score, dim=1).squeeze()\n",
    "\n",
    "        # L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]\n",
    "        final_score = score + neg_score\n",
    "        loss = -1 * torch.sum(final_score)\n",
    "        return loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.w_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = SkipGramModel(100, 10)\n",
    "    id2word = dict()\n",
    "    for i in range(100):\n",
    "        id2word[i] = str(i)\n",
    "    pos_w = [0, 2]\n",
    "    pos_v = [[9,10],[10,12]]\n",
    "    neg_v = [[23, 42, 74, 32], [32, 24, 62, 53]]\n",
    "    model.forward(pos_w, pos_v, neg_v)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1639850405570,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "z6pcTUByZP0D"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 2 \n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.01 # Learning rate\n",
    "NEG_COUNT = 12\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, sentences):\n",
    "        self.data = InputData(sentences)\n",
    "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"SkipGram Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,51):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE, NEG_COUNT)\n",
    "                pos_w = [int(pair[0]) for pair in pos_pairs]\n",
    "                pos_v = [pair[1] for pair in pos_pairs]\n",
    "                neg_v = [pair[2] for pair in pos_pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_w, pos_v, neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     w2v = Word2Vec(input_file_name='./data.txt', output_file_name=\"word_embedding.txt\")\n",
    "#     w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12904,
     "status": "ok",
     "timestamp": 1639850423748,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "qBk19rOeBHAs",
    "outputId": "22323f41-0e25-4e9d-d071-2ec272f49793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100002426,)\n",
      "Word Count is: 24758\n",
      "Word Count Sum is 313120\n",
      "Sentence Count is: 17525\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "w2v = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1217820,
     "status": "ok",
     "timestamp": 1639851648826,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "jIkOnp9GETIS",
    "outputId": "eee9e9bd-7779-4b75-e61f-bee555b9d0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Training......\n",
      "pairs_count 313120\n",
      "batch_count 313.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(10559.4941, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(9552.1973, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:25<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(9081.1162, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(8703.0986, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(8543.0039, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(8271.6953, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(8199.7383, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(7927.1582, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(7886.7163, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(7592.1196, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(7562.4355, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(7249.7588, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(7222.5439, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(6905.2021, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(6869.1694, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(6571.0273, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(6526.8955, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(6242.3540, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(6200.8833, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(5939.1787, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 loss: tensor(5895.2720, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 loss: tensor(5658.0757, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 loss: tensor(5615.2202, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 loss: tensor(5398.9575, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 loss: tensor(5364.9268, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 loss: tensor(5167.7378, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 loss: tensor(5129.7109, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 loss: tensor(4964.8901, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: tensor(4923.8481, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: tensor(4775.6602, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 loss: tensor(4736.5415, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss: tensor(4601.7192, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 loss: tensor(4569.3848, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 loss: tensor(4456.5142, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:25<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 loss: tensor(4419.6387, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 loss: tensor(4317.5361, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 loss: tensor(4282.6782, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 loss: tensor(4201.4062, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 loss: tensor(4163.0273, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: tensor(4097.1724, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:25<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 loss: tensor(4053.0471, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 loss: tensor(4000.3977, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 loss: tensor(3958.9170, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 loss: tensor(3913.2500, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 loss: tensor(3871.3762, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:24<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 loss: tensor(3841.8474, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:26<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 loss: tensor(3790.5144, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 loss: tensor(3772.9133, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss: tensor(3729.4109, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(3709.0132, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 15757,
     "status": "ok",
     "timestamp": 1639851669793,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "GYx4RpJkIzU_"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1639852458944,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "TOK9m2aBI79I",
    "outputId": "4f38ad28-d2d5-4fc5-95f7-080be95c4228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ballot': ['referendum',\n",
       "  'nominating',\n",
       "  'nonpartisan',\n",
       "  'subcommittee',\n",
       "  'gangland',\n",
       "  'miscount',\n",
       "  'retirements',\n",
       "  'welled',\n",
       "  'buddies'],\n",
       " 'car': ['collided',\n",
       "  'streetcar',\n",
       "  'heading',\n",
       "  'drives',\n",
       "  'pezza',\n",
       "  'renting',\n",
       "  'morals',\n",
       "  'headlights',\n",
       "  'backward'],\n",
       " 'election': ['induction',\n",
       "  'commenting',\n",
       "  'seconddegree',\n",
       "  'burglary',\n",
       "  'races',\n",
       "  'beveling',\n",
       "  'overexpose',\n",
       "  'regattas',\n",
       "  'calmest'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'bockwurst',\n",
       "  'niger',\n",
       "  'stewardesses',\n",
       "  'knackwurst',\n",
       "  'harelips',\n",
       "  'apergillus',\n",
       "  'flavus',\n",
       "  'copland'],\n",
       " 'player': ['birdied',\n",
       "  'rosburg',\n",
       "  'reportedly',\n",
       "  'lewisohn',\n",
       "  'dookiyoon',\n",
       "  'invented',\n",
       "  'tee',\n",
       "  'chapters',\n",
       "  'babe'],\n",
       " 'republican': ['nominee',\n",
       "  'partys',\n",
       "  'dirksen',\n",
       "  'gubernatorial',\n",
       "  'hillel',\n",
       "  'leverett',\n",
       "  'distraught',\n",
       "  'forum',\n",
       "  'independents'],\n",
       " 'sauce': ['pineapple',\n",
       "  'tablespoon',\n",
       "  'sweetsour',\n",
       "  'teaspoon',\n",
       "  'savory',\n",
       "  'chunks',\n",
       "  'worcestershire',\n",
       "  'sauerkraut',\n",
       "  'horseradish'],\n",
       " 'tablespoon': ['teaspoon',\n",
       "  'teaspoons',\n",
       "  'worcestershire',\n",
       "  'vinegar',\n",
       "  'walnuts',\n",
       "  'chopped',\n",
       "  'gypsum',\n",
       "  'ketchup',\n",
       "  'celery'],\n",
       " 'university': ['astronomy',\n",
       "  'kennel',\n",
       "  'bachelor',\n",
       "  'southwestern',\n",
       "  'pharmacy',\n",
       "  'faculties',\n",
       "  'inspector',\n",
       "  'chicagos',\n",
       "  'uninterrupted']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','election','sauce', 'ballot','mettwurst','car','player','university','republican']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNmaQk6Vh4m89Vi9A7DyCn2",
   "collapsed_sections": [],
   "name": "Word2Vec - skipGram with negative sampling and subsampling.ipynb",
   "provenance": [
    {
     "file_id": "1caWX3u6zHAL8KviSlakIKe9U54-Bqe54",
     "timestamp": 1639833373580
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
