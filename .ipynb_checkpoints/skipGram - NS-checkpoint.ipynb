{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10885,
     "status": "ok",
     "timestamp": 1639733931411,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "TKbGaf5uYpaD",
    "outputId": "68d22c43-a1a2-4daa-cedc-5ea6538d6d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(100000004,)\n",
      "Word Count is: 29\n",
      "Word Count Sum is 34\n",
      "Sentence Count is: 2\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "positive:\n",
      "[(0, [29, 29, 1, 2], [6, 23, 2, 2, 6, 14, 3, 28]), (1, [29, 0, 2, 3], [12, 6, 1, 18, 21, 23, 11, 4]), (2, [0, 1, 3, 4], [10, 13, 6, 20, 10, 11, 14, 10]), (3, [1, 2, 4, 5], [24, 4, 19, 23, 10, 20, 10, 13]), (4, [2, 3, 5, 6], [23, 6, 15, 3, 11, 4, 22, 18]), (5, [3, 4, 6, 7], [10, 10, 20, 13, 9, 10, 4, 6]), (6, [4, 5, 7, 8], [18, 12, 4, 2, 22, 17, 5, 14]), (7, [5, 6, 8, 9], [6, 20, 5, 9, 13, 27, 0, 21]), (8, [6, 7, 9, 10], [6, 25, 2, 18, 10, 10, 5, 22]), (9, [7, 8, 10, 11], [2, 21, 22, 12, 10, 12, 12, 13])]\n",
      "[('fulton', ['county', 'grand'], ['investigation', 'deserves', 'grand', 'grand', 'investigation', 'took', 'jury', 'conducted']), ('county', ['fulton', 'grand', 'jury'], ['evidence', 'investigation', 'county', 'city', 'overall', 'deserves', 'produced', 'said']), ('grand', ['fulton', 'county', 'jury', 'said'], ['election', 'irregularities', 'investigation', 'committee', 'election', 'produced', 'took', 'election']), ('jury', ['county', 'grand', 'said', 'friday'], ['praise', 'said', 'executive', 'deserves', 'election', 'committee', 'election', 'irregularities']), ('said', ['grand', 'jury', 'friday', 'investigation'], ['deserves', 'investigation', 'place', 'jury', 'produced', 'said', 'charge', 'city']), ('friday', ['jury', 'said', 'investigation', 'atlantas'], ['election', 'election', 'committee', 'irregularities', 'primary', 'election', 'said', 'investigation']), ('investigation', ['said', 'friday', 'atlantas', 'recent'], ['city', 'evidence', 'said', 'grand', 'charge', 'presentments', 'friday', 'took']), ('atlantas', ['friday', 'investigation', 'recent', 'primary'], ['investigation', 'committee', 'friday', 'primary', 'irregularities', 'manner', 'fulton', 'overall']), ('recent', ['investigation', 'atlantas', 'primary', 'election'], ['investigation', 'thanks', 'grand', 'city', 'election', 'election', 'friday', 'charge']), ('primary', ['atlantas', 'recent', 'election', 'produced'], ['grand', 'overall', 'charge', 'evidence', 'election', 'evidence', 'evidence', 'irregularities'])]\n",
      "10\n",
      "positive:\n",
      "[(10, [8, 9, 11, 12], [17, 27, 13, 12, 3, 6, 23, 15]), (11, [9, 10, 12, 13], [4, 25, 14, 21, 15, 4, 6, 20]), (12, [10, 11, 13, 14], [18, 4, 18, 27, 17, 3, 21, 2]), (13, [11, 12, 14, 15], [10, 23, 28, 22, 24, 21, 22, 23]), (14, [12, 13, 15, 29], [9, 26, 5, 5, 9, 9, 20, 19]), (15, [13, 14, 29, 29], [4, 11, 12, 16, 3, 9, 22, 22]), (3, [29, 29, 4, 16], [1, 3, 17, 24, 19, 17, 28, 1]), (4, [29, 3, 16, 17], [18, 7, 18, 10, 27, 3, 3, 4]), (16, [3, 4, 17, 18], [3, 27, 9, 27, 16, 10, 5, 12]), (17, [4, 16, 18, 19], [6, 26, 27, 4, 15, 22, 13, 18])]\n",
      "[('election', ['recent', 'primary', 'produced', 'evidence'], ['presentments', 'manner', 'irregularities', 'evidence', 'jury', 'investigation', 'deserves', 'place']), ('produced', ['primary', 'election', 'evidence', 'irregularities'], ['said', 'thanks', 'took', 'overall', 'place', 'said', 'investigation', 'committee']), ('evidence', ['election', 'produced', 'irregularities', 'took'], ['city', 'said', 'city', 'manner', 'presentments', 'jury', 'overall', 'grand']), ('irregularities', ['produced', 'evidence', 'took', 'place'], ['election', 'deserves', 'conducted', 'charge', 'praise', 'overall', 'charge', 'deserves']), ('took', ['evidence', 'irregularities', 'place'], ['primary', 'atlanta', 'friday', 'friday', 'primary', 'primary', 'committee', 'executive']), ('place', ['irregularities', 'took'], ['said', 'produced', 'evidence', 'termend', 'jury', 'primary', 'charge', 'charge']), ('jury', ['said', 'termend'], ['county', 'jury', 'presentments', 'praise', 'executive', 'presentments', 'conducted', 'county']), ('said', ['jury', 'termend', 'presentments'], ['city', 'atlantas', 'city', 'election', 'manner', 'jury', 'jury', 'said']), ('termend', ['jury', 'said', 'presentments', 'city'], ['jury', 'manner', 'primary', 'manner', 'termend', 'election', 'friday', 'evidence']), ('presentments', ['said', 'termend', 'city', 'executive'], ['investigation', 'atlanta', 'manner', 'said', 'place', 'charge', 'irregularities', 'city'])]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.normalize()\n",
    "        self.counter = 0\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  #  Number of words (repeated words only count as 1)\n",
    "        self.word_count_sum = 0  # Total number of words (the number of repeated words also accumulates)\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict()  # Initialize the dictionary\n",
    "        self.sample_table = []\n",
    "        self._init_sample_table()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "\n",
    "    def normalize(self):\n",
    "      stop_words = nltk.corpus.stopwords.words('english')\n",
    "      norm_sentences_word_list = []\n",
    "      for word_list in self.sentences:\n",
    "        sentence = \" \".join(word for word in word_list)\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(' +', ' ', sentence)\n",
    "        sentence = sentence.strip()\n",
    "        norm_word_list = sentence.split(' ')\n",
    "        norm_word_list = [word for word in norm_word_list if word not in stop_words]\n",
    "        if(len(norm_word_list) > 1):\n",
    "          norm_sentences_word_list.append(norm_word_list)\n",
    "       \n",
    "      self.sentences = norm_sentences_word_list\n",
    "\n",
    "    def _init_dict(self):\n",
    "        word_freq = dict()\n",
    "        for word_list in self.sentences:\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "        word_id = 0\n",
    "        # Initialize word2id_dict, id2word_dict, wordId_frequency_dict dictionary\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def _init_sample_table(self):\n",
    "        sample_table_size = 1e8\n",
    "        frequency = np.array(list(self.wordId_frequency_dict.values())) ** 0.75\n",
    "        frequency_sum = sum(frequency)\n",
    "        ratio_array = frequency / frequency_sum \n",
    "        word_count_list = np.round(ratio_array * sample_table_size)\n",
    "        for word_index, word_freq in enumerate(word_count_list):\n",
    "            self.sample_table += [word_index] * int(word_freq)  # it generates a list, the content is the id of each word, each id in the list is repeated multiple times, the number of repetitions is the word frequency\n",
    "        self.sample_table = np.array(self.sample_table)\n",
    "        print(self.sample_table.shape)\n",
    "\n",
    "    def generate_positive_pairs(self, window_size, neg_count):\n",
    "        self.counter += 1\n",
    "        if not self.sentences[20*(self.counter-1):20*self.counter]:\n",
    "            self.counter = 1\n",
    "            self.word_pairs_queue.clear()\n",
    "        sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.sentences[20*(self.counter-1):20*self.counter]]\n",
    "\n",
    "        \n",
    "        for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, center_word in enumerate(words):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            positive_words = []\n",
    "            for index_2 in range(start,end):\n",
    "              if 0 <= index_2 < sentence_length and index_2 != index:\n",
    "                positive_words.append(words[index_2])\n",
    "              elif index_2 < 0 or index_2 >= sentence_length:\n",
    "                positive_words.append(self.word_count)\n",
    "            \n",
    "            negative_words = np.random.choice(self.sample_table, size=neg_count).tolist()\n",
    "\n",
    "            self.word_pairs_queue.append((center_word, positive_words, negative_words))\n",
    "           \n",
    "\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size, neg_count):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_positive_pairs(window_size, neg_count)              \n",
    "              \n",
    "        result_pairs = []\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])[:2]\n",
    "    test_data = InputData(sentences)\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], [test_data.id2word_dict[i] for i in pair[1] if i != test_data.word_count], [test_data.id2word_dict[i] for i in pair[2] if i != test_data.word_count]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], [test_data.id2word_dict[i] for i in pair[1] if i != test_data.word_count], [test_data.id2word_dict[i] for i in pair[2] if i != test_data.word_count]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1639737228342,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "UunHC6-rZBpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.w_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size + 1, emb_dimension, sparse=True)\n",
    "        self._init_emb()\n",
    "\n",
    "    def _init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_w, pos_v, neg_v):\n",
    "        emb_w = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "        emb_v = self.v_embeddings(torch.LongTensor(pos_v))\n",
    "        neg_emb_v = self.v_embeddings(torch.LongTensor(neg_v))\n",
    "\n",
    "        score = torch.mul(emb_w.unsqueeze(1), emb_v)\n",
    "        score = torch.sum(score, dim=2).squeeze()\n",
    "        score = F.logsigmoid(score)\n",
    "        score = torch.sum(score, dim=1).squeeze()\n",
    "\n",
    "        neg_score = torch.mul(emb_w.unsqueeze(1), neg_emb_v)\n",
    "        neg_score = torch.sum(neg_score, dim=2).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        neg_score = torch.sum(neg_score, dim=1).squeeze()\n",
    "\n",
    "\n",
    "        final_score = score + neg_score\n",
    "        loss = -1 * torch.sum(final_score)\n",
    "        return loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.w_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = SkipGramModel(100, 10)\n",
    "    id2word = dict()\n",
    "    for i in range(100):\n",
    "        id2word[i] = str(i)\n",
    "    pos_w = [0, 2]\n",
    "    pos_v = [[9,10],[10,12]]\n",
    "    neg_v = [[23, 42, 74, 32], [32, 24, 62, 53]]\n",
    "    model.forward(pos_w, pos_v, neg_v)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1639739299505,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "z6pcTUByZP0D"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 2 \n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.01 # Learning rate\n",
    "NEG_COUNT = 8\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, sentences):\n",
    "        self.data = InputData(sentences)\n",
    "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"SkipGram Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,51):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE, NEG_COUNT)\n",
    "                pos_w = [int(pair[0]) for pair in pos_pairs]\n",
    "                pos_v = [pair[1] for pair in pos_pairs]\n",
    "                neg_v = [pair[2] for pair in pos_pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_w, pos_v, neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12899,
     "status": "ok",
     "timestamp": 1639739317676,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "qBk19rOeBHAs",
    "outputId": "11e4eb6d-d46b-4c46-98f3-a92dc24d8a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999555,)\n",
      "Word Count is: 24616\n",
      "Word Count Sum is 170964\n",
      "Sentence Count is: 17106\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "w2v = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 701103,
     "status": "ok",
     "timestamp": 1639740034354,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "jIkOnp9GETIS",
    "outputId": "b6c7856b-ba5d-4880-875a-a11a9891e4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Training......\n",
      "pairs_count 170964\n",
      "batch_count 170.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(8198.7705, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(7923.1514, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(7771.2690, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(7648.2461, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(7530.7876, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(7426.8081, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(7336.6836, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(7256.5771, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(7185.5435, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(7115.2905, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(7045.4785, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(6975.2412, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(6903.3096, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(6833.1528, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(6760.4873, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(6688.8193, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(6618.3057, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(6545.3223, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(6472.0903, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(6400.4536, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 loss: tensor(6325.2568, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 loss: tensor(6252.9956, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 loss: tensor(6173.2905, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 loss: tensor(6097.5396, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 loss: tensor(6016.7954, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 loss: tensor(5934.8726, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 loss: tensor(5850.3979, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 loss: tensor(5767.6050, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: tensor(5681.5376, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: tensor(5591.8013, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 loss: tensor(5505.8101, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss: tensor(5419.8413, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 loss: tensor(5331.1855, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 loss: tensor(5241.4663, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 loss: tensor(5150.7241, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 loss: tensor(5057.0308, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 loss: tensor(4968.7158, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 loss: tensor(4878.4702, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 loss: tensor(4788.6514, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: tensor(4701.3857, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 loss: tensor(4617.2241, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 loss: tensor(4522.9956, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 loss: tensor(4443.1123, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 loss: tensor(4351.4858, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 loss: tensor(4270.9604, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 loss: tensor(4189.8584, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 loss: tensor(4110.1616, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 loss: tensor(4028.9639, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss: tensor(3951.3306, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(3882.2522, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 18288,
     "status": "ok",
     "timestamp": 1639740416229,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "GYx4RpJkIzU_"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1639740430650,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "TOK9m2aBI79I",
    "outputId": "2cd5e26f-538f-4017-d522-3a1b5f43a52b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'car': ['renting',\n",
       "  'driven',\n",
       "  'passing',\n",
       "  'driver',\n",
       "  'loaded',\n",
       "  'driving',\n",
       "  'rides',\n",
       "  'gloriana',\n",
       "  'pounds'],\n",
       " 'children': ['teach',\n",
       "  'daddy',\n",
       "  'neighbors',\n",
       "  'hundreds',\n",
       "  'fronts',\n",
       "  'relatives',\n",
       "  'loved',\n",
       "  'fortunate',\n",
       "  'helpless'],\n",
       " 'democratic': ['gubernatorial',\n",
       "  'invitation',\n",
       "  'buckley',\n",
       "  'republican',\n",
       "  'socialist',\n",
       "  'candidate',\n",
       "  'democrats',\n",
       "  'elected',\n",
       "  'statesman'],\n",
       " 'election': ['votes',\n",
       "  'defeated',\n",
       "  'ballot',\n",
       "  'bush',\n",
       "  'socialist',\n",
       "  'gubernatorial',\n",
       "  'contention',\n",
       "  'bradley',\n",
       "  'algol'],\n",
       " 'football': ['teen',\n",
       "  'clubs',\n",
       "  'cincinnati',\n",
       "  'champions',\n",
       "  'miami',\n",
       "  'buffalo',\n",
       "  'minnesota',\n",
       "  'presents',\n",
       "  'basketball'],\n",
       " 'game': ['longhorns',\n",
       "  'lewisohn',\n",
       "  'stadium',\n",
       "  'eighth',\n",
       "  'begun',\n",
       "  'playoff',\n",
       "  'pitching',\n",
       "  'knee',\n",
       "  'weve'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'bologna',\n",
       "  'psyche',\n",
       "  'blacking',\n",
       "  'depressants',\n",
       "  'bockwurst',\n",
       "  'rodneymiss',\n",
       "  'copland',\n",
       "  'stares'],\n",
       " 'sauce': ['tablespoon',\n",
       "  'mustard',\n",
       "  'tablespoons',\n",
       "  'worcestershire',\n",
       "  'teaspoon',\n",
       "  'chili',\n",
       "  'pineapple',\n",
       "  'teaspoons',\n",
       "  'paste'],\n",
       " 'tablespoon': ['teaspoons',\n",
       "  'worcestershire',\n",
       "  'teaspoon',\n",
       "  'vinegar',\n",
       "  'sweetsour',\n",
       "  'chili',\n",
       "  'sauerkraut',\n",
       "  'tablespoons',\n",
       "  'melted'],\n",
       " 'university': ['carleton',\n",
       "  'awarded',\n",
       "  'emory',\n",
       "  'professional',\n",
       "  'institute',\n",
       "  'physics',\n",
       "  'graduate',\n",
       "  'hunter',\n",
       "  'magazine']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','election','sauce', 'democratic','game','children','mettwurst','car','football','university']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZcB4r+1H4Yhvbm1P9zR0i",
   "collapsed_sections": [],
   "name": "Word2Vec - skipGram with negative sampling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
