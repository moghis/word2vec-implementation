{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1639757115066,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "SOT0vcq8GhLw",
    "outputId": "73b91d0c-3abc-4c1e-957e-789a0def53f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n",
      "{1: [1, 1], 0: [1, 0], 3: [0, 1, 1], 4: [0, 1, 0], 2: [0, 0]}\n",
      "{1: [8, 7], 0: [8, 7], 3: [8, 6, 5], 4: [8, 6, 5], 2: [8, 6]}\n",
      "[8, 7]\n",
      "[8, 7]\n",
      "[8, 6]\n",
      "[8, 6, 5]\n",
      "[8, 6, 5]\n",
      "([[8], [8, 7], [], [6, 5], [6]], [[7], [], [8, 6], [8], [8, 5]])\n"
     ]
    }
   ],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, word_id, frequency):\n",
    "        self.word_id = word_id\n",
    "        self.frequency = frequency\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.father = None\n",
    "        self.Huffman_code = []\n",
    "        self.path = []\n",
    "\n",
    "\n",
    "class HuffmanTree:\n",
    "    def __init__(self, wordid_frequency_dict):\n",
    "        self.word_count = len(wordid_frequency_dict)\n",
    "        self.wordid_code = dict()\n",
    "        self.wordid_path = dict()\n",
    "        self.root = None\n",
    "        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] # Unmerged node list\n",
    "        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] # Store all leaf nodes and intermediate nodes\n",
    "        # Build huffman tree\n",
    "        self.build_tree(unmerge_node_list)\n",
    "        # Generate huffman code\n",
    "        self.generate_huffman_code_and_path()\n",
    "\n",
    "    def merge_node(self, node1, node2):\n",
    "        sum_frequency = node1.frequency + node2.frequency\n",
    "        mid_node_id = len(self.huffman)\n",
    "        father_node = HuffmanNode(mid_node_id, sum_frequency)\n",
    "        if node1.frequency >= node2.frequency:\n",
    "            father_node.left_child = node1\n",
    "            father_node.right_child = node2\n",
    "        else:\n",
    "            father_node.left_child = node2\n",
    "            father_node.right_child = node1\n",
    "        self.huffman.append(father_node)\n",
    "        return father_node\n",
    "\n",
    "    def build_tree(self, node_list):\n",
    "        while len(node_list) > 1:\n",
    "            i1 = 0  # Node with least frequency\n",
    "            i2 = 1  # Node with the second smallest frequency\n",
    "            if node_list[i2].frequency < node_list[i1].frequency:\n",
    "                [i1, i2] = [i2, i1]\n",
    "            for i in range(2, len(node_list)):\n",
    "                if node_list[i].frequency < node_list[i2].frequency:\n",
    "                    i2 = i\n",
    "                    if node_list[i2].frequency < node_list[i1].frequency:\n",
    "                        [i1, i2] = [i2, i1]\n",
    "            father_node = self.merge_node(node_list[i1], node_list[i2])  # Combine the least frequent two nodes\n",
    "            if i1 < i2:\n",
    "                node_list.pop(i2)\n",
    "                node_list.pop(i1)\n",
    "            elif i1 > i2:\n",
    "                node_list.pop(i1)\n",
    "                node_list.pop(i2)\n",
    "            else:\n",
    "                raise RuntimeError('i1 should not be equal to i2')\n",
    "            node_list.insert(0, father_node)  # Insert new node\n",
    "        self.root = node_list[0]\n",
    "\n",
    "    def generate_huffman_code_and_path(self):\n",
    "        stack = [self.root]\n",
    "        while len(stack) > 0:\n",
    "            node = stack.pop()\n",
    "            while node.left_child or node.right_child:\n",
    "                code = node.Huffman_code\n",
    "                path = node.path\n",
    "                node.left_child.Huffman_code = code + [1]\n",
    "                node.right_child.Huffman_code = code + [0]\n",
    "                node.left_child.path = path + [node.word_id]\n",
    "                node.right_child.path = path + [node.word_id]\n",
    "                stack.append(node.right_child)\n",
    "                node = node.left_child\n",
    "            word_id = node.word_id\n",
    "            word_code = node.Huffman_code\n",
    "            word_path = node.path\n",
    "            self.huffman[word_id].Huffman_code = word_code\n",
    "            self.huffman[word_id].path = word_path\n",
    "            # Write the Huffman code and path calculated by the node into the value of the dictionary\n",
    "            self.wordid_code[word_id] = word_code\n",
    "            self.wordid_path[word_id] = word_path\n",
    "\n",
    "    # Get the id of all positive nodes and the id of all negative nodes\n",
    "    def get_all_pos_and_neg_path(self):\n",
    "        positive = []  # Array of positive paths of all words\n",
    "        negative = []  # Array of negative paths for all words\n",
    "        for word_id in range(self.word_count):\n",
    "            pos_id = []\n",
    "            neg_id = []\n",
    "            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n",
    "                if code == 1:\n",
    "                    pos_id.append(self.huffman[word_id].path[i])\n",
    "                else:\n",
    "                    neg_id.append(self.huffman[word_id].path[i])\n",
    "            positive.append(pos_id)\n",
    "            negative.append(neg_id)\n",
    "        return positive, negative\n",
    "\n",
    "\n",
    "def test():\n",
    "    word_frequency = {0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n",
    "    print(word_frequency)\n",
    "    tree = HuffmanTree(word_frequency)\n",
    "    print(tree.wordid_code)\n",
    "    print(tree.wordid_path)\n",
    "    for i in range(len(word_frequency)):\n",
    "        print(tree.huffman[i].path)\n",
    "    print(tree.get_all_pos_and_neg_path())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1639757119984,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "CEaJ1tAeRWYh",
    "outputId": "f6df0c9c-99ae-4244-e0e0-09e8b46673de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Word Count is: 29\n",
      "Word Count Sum is 34\n",
      "Sentence Count is: 2\n",
      "Tree Node is: 57\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "[(0, [1, 2]), (1, [0, 2, 3]), (2, [0, 1, 3, 4]), (3, [1, 2, 4, 5]), (4, [2, 3, 5, 6]), (5, [3, 4, 6, 7]), (6, [4, 5, 7, 8]), (7, [5, 6, 8, 9]), (8, [6, 7, 9, 10]), (9, [7, 8, 10, 11])]\n",
      "[('fulton', ['county', 'grand']), ('county', ['fulton', 'grand', 'jury']), ('grand', ['fulton', 'county', 'jury', 'said']), ('jury', ['county', 'grand', 'said', 'friday']), ('said', ['grand', 'jury', 'friday', 'investigation']), ('friday', ['jury', 'said', 'investigation', 'atlantas']), ('investigation', ['said', 'friday', 'atlantas', 'recent']), ('atlantas', ['friday', 'investigation', 'recent', 'primary']), ('recent', ['investigation', 'atlantas', 'primary', 'election']), ('primary', ['atlantas', 'recent', 'election', 'produced'])]\n",
      "[56, 47, 29]\n",
      "[55, 50]\n",
      "[56, 47]\n",
      "[55, 50, 29]\n",
      "[51, 30]\n",
      "[56, 54, 46]\n",
      "[(0, 56), (0, 47), (0, 51), (0, 30), (1, 56), (1, 47), (1, 29), (1, 51), (1, 30), (1, 56), (2, 56), (2, 47), (2, 29), (2, 56), (2, 47), (2, 56), (2, 56), (2, 50), (2, 48), (3, 56), (3, 47), (3, 51), (3, 30), (3, 56), (3, 50), (3, 48), (3, 51), (4, 51), (4, 30), (4, 56), (4, 51), (4, 51), (4, 46), (4, 31), (5, 56), (5, 56), (5, 50), (5, 48), (5, 51), (5, 46), (5, 31), (5, 51), (5, 46), (6, 56), (6, 50), (6, 48), (6, 51), (6, 51), (6, 46), (6, 32), (7, 51), (7, 51), (7, 46), (7, 31), (7, 32), (8, 51), (8, 46), (8, 31), (8, 51), (8, 46), (8, 56), (8, 55), (8, 53), (9, 51), (9, 46), (9, 32), (9, 56), (9, 55), (9, 53), (9, 45), (9, 33)]\n",
      "[(0, 55), (0, 50), (0, 29), (0, 56), (0, 54), (0, 46), (1, 55), (1, 50), (1, 56), (1, 54), (1, 46), (1, 55), (1, 50), (1, 47), (2, 55), (2, 50), (2, 55), (2, 50), (2, 29), (2, 55), (2, 50), (2, 47), (2, 55), (3, 55), (3, 50), (3, 29), (3, 56), (3, 54), (3, 46), (3, 55), (3, 56), (3, 54), (3, 46), (3, 30), (4, 56), (4, 54), (4, 46), (4, 55), (4, 50), (4, 47), (4, 56), (4, 54), (4, 46), (4, 30), (4, 56), (4, 54), (5, 55), (5, 50), (5, 47), (5, 55), (5, 56), (5, 54), (5, 56), (5, 54), (5, 31), (6, 55), (6, 56), (6, 54), (6, 46), (6, 30), (6, 56), (6, 54), (6, 31), (6, 56), (6, 54), (6, 51), (6, 45), (7, 56), (7, 54), (7, 46), (7, 30), (7, 56), (7, 54), (7, 56), (7, 54), (7, 51), (7, 45), (7, 56), (7, 54), (7, 51), (7, 45), (7, 32), (8, 56), (8, 54), (8, 56), (8, 54), (8, 31), (8, 56), (8, 54), (8, 51), (8, 45), (8, 32), (8, 49), (9, 56), (9, 54), (9, 31), (9, 56), (9, 54), (9, 51), (9, 45), (9, 49), (9, 56), (9, 54), (9, 51)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.normalize()\n",
    "        self.counter = 0\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  # Number of words (repeated words only count as 1)\n",
    "        self.word_count_sum = 0  # Total number of words (the number of repeated words also accumulates)\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict()  # Initialize the dictionary\n",
    "        self.huffman_tree = HuffmanTree(self.wordId_frequency_dict)  # Hoffman Tree\n",
    "        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "        print('Tree Node is:', len(self.huffman_tree.huffman))\n",
    "\n",
    "    def normalize(self):\n",
    "      stop_words = nltk.corpus.stopwords.words('english')\n",
    "      norm_sentences_word_list = []\n",
    "      for word_list in self.sentences:\n",
    "        sentence = \" \".join(word for word in word_list)\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(' +', ' ', sentence)\n",
    "        sentence = sentence.strip()\n",
    "        norm_word_list = sentence.split(' ')\n",
    "        norm_word_list = [word for word in norm_word_list if word not in stop_words]\n",
    "        if(len(norm_word_list) > 1):\n",
    "          norm_sentences_word_list.append(norm_word_list)\n",
    "       \n",
    "      self.sentences = norm_sentences_word_list\n",
    "\n",
    "    def _init_dict(self):\n",
    "        word_freq = dict()\n",
    "        for word_list in self.sentences:\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "        word_id = 0\n",
    "        # Initialize word2id_dict, id2word_dict, wordId_frequency_dict dictionary\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def generate_center_context_pairs(self, window_size):\n",
    "        self.counter += 1\n",
    "        if not self.sentences[20*(self.counter-1):20*self.counter]:\n",
    "            self.counter = 1\n",
    "            self.word_pairs_queue.clear()\n",
    "        sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.sentences[20*(self.counter-1):20*self.counter]]\n",
    "\n",
    "\n",
    "        for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, center_word in enumerate(words):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words = []\n",
    "            for index_2 in range(start,end):\n",
    "              if 0 <= index_2 < sentence_length and index_2 != index:\n",
    "                context_words.append(words[index_2])\n",
    "\n",
    "            self.word_pairs_queue.append((center_word, context_words))           \n",
    "\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_center_context_pairs(window_size)              \n",
    "              \n",
    "        result_pairs = []  # Returns a positive sample pair of mini-batch size\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    def get_pos_neg_pairs(self, pairs):\n",
    "        neg_word_pair = []\n",
    "        pos_word_pair = []\n",
    "        for pair in pairs:\n",
    "            for context_word in pair[1]:\n",
    "              pos_word_pair += zip([pair[0]] * len(self.huffman_pos_path[context_word]), self.huffman_pos_path[context_word])\n",
    "              neg_word_pair += zip([pair[0]] * len(self.huffman_neg_path[context_word]), self.huffman_neg_path[context_word])\n",
    "        return pos_word_pair, neg_word_pair\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])[:2]\n",
    "    test_data = InputData(sentences)\n",
    "\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "\n",
    "    \n",
    "    batch_pairs = test_data.get_batch_pairs(10, 2)\n",
    "    pos_pairs, neg_pairs = test_data.get_pos_neg_pairs(batch_pairs)\n",
    "    batch_word_pairs = []\n",
    "    for pair in batch_pairs:\n",
    "        batch_word_pairs.append((test_data.id2word_dict[pair[0]] , [test_data.id2word_dict[i] for i in pair[1]]))\n",
    "    print(batch_pairs)\n",
    "    print(batch_word_pairs)\n",
    "    \n",
    "    print(test_data.huffman_pos_path[0])\n",
    "    print(test_data.huffman_neg_path[0])\n",
    "    print(test_data.huffman_pos_path[1])\n",
    "    print(test_data.huffman_neg_path[1])\n",
    "    print(test_data.huffman_pos_path[2])\n",
    "    print(test_data.huffman_neg_path[2])\n",
    "\n",
    "    print(pos_pairs)\n",
    "    print(neg_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1639757426707,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "vIC4eGjs8SZ9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(2*emb_size-1, emb_dimension, sparse=True)\n",
    "        self.w_embeddings = nn.Embedding(2*emb_size-1, emb_dimension, sparse=True)\n",
    "        self._init_emb()\n",
    "\n",
    "    def _init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.w_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_u, pos_w, neg_u, neg_w):\n",
    "        pos_u_emb = self.u_embeddings(torch.LongTensor(pos_u))\n",
    "        pos_w_emb = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "\n",
    "        neg_u_emb = self.u_embeddings(torch.LongTensor(neg_u))\n",
    "        neg_w_emb = self.w_embeddings(torch.LongTensor(neg_w))\n",
    "\n",
    "        score = torch.mul(pos_u_emb, pos_w_emb)\n",
    "        score = torch.sum(score, dim=1).squeeze()\n",
    "        score = F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.mul(neg_u_emb, neg_w_emb)\n",
    "        neg_score = torch.sum(neg_score, dim=1).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "\n",
    "        # L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]\n",
    "        loss = -1 * (torch.sum(score) + torch.sum(neg_score)) \n",
    "        return loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.u_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = SkipGramModel(100, 10)\n",
    "    id2word = dict()\n",
    "    for i in range(100):\n",
    "        id2word[i] = str(i)\n",
    "    pos_u = [0, 0, 1, 1, 1]\n",
    "    pos_w = [102, 134, 173, 183, 148]\n",
    "    neg_u = [0, 0, 0, 1, 1]\n",
    "    neg_w = [123, 166, 192, 111, 177]\n",
    "    model.forward(pos_u, pos_w, neg_u, neg_w)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1639764020890,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "OKt0clpZGPtP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 2 \n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.01  # Learning rate\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self,sentences):\n",
    "        self.data = InputData(sentences)\n",
    "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        # lambda1 = lambda epoch: 0.99 ** epoch\n",
    "        # self.scheduler = SkipGramModel(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"CBOW Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,51):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
    "                pos_pairs, neg_pairs = self.data.get_pos_neg_pairs(pairs)\n",
    "\n",
    "                pos_u = [int(pair[0]) for pair in pos_pairs]\n",
    "                pos_w = [int(pair[1]) for pair in pos_pairs]\n",
    "                neg_u = [int(pair[0]) for pair in neg_pairs]\n",
    "                neg_w = [int(pair[1]) for pair in neg_pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u, pos_w, neg_u, neg_w)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "            # self.scheduler.step()\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66661,
     "status": "ok",
     "timestamp": 1639764090519,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "29w2zJngIMRu",
    "outputId": "3e26d6ab-2c4d-46ad-c55b-aa7b026de955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count is: 24616\n",
      "Word Count Sum is 170964\n",
      "Sentence Count is: 17106\n",
      "Tree Node is: 49231\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "w2v = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1102461,
     "status": "ok",
     "timestamp": 1639765201142,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "vF7oq1M-IN3N",
    "outputId": "fd6d4607-e153-4d21-dd5b-42d0ec4c2178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Training......\n",
      "pairs_count 170964\n",
      "batch_count 170.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(30116.2832, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(30063.9785, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(29988.3184, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(29846.7383, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(29642.0625, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(29388.5801, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(29101.3672, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(28798.4316, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(28460.7539, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(28110.1055, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(27745.3301, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(27404.9316, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(27029.7832, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(26695.7207, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(26369.5000, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(26082.0879, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(25759.9062, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(25514.3828, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(25225.1621, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(24966.1230, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 loss: tensor(24683.2441, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 loss: tensor(24471.6055, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 loss: tensor(24308.5449, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 loss: tensor(24060.8203, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 loss: tensor(23828.8418, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 loss: tensor(23706.4355, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 loss: tensor(23535.8652, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 loss: tensor(23390.1582, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: tensor(23248.7500, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: tensor(23107.1230, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 loss: tensor(22943.2598, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss: tensor(22784.6816, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 loss: tensor(22682.2090, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 loss: tensor(22459.6348, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 loss: tensor(22387.0176, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 loss: tensor(22295.2441, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 loss: tensor(22174.6113, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 loss: tensor(22093.9062, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 loss: tensor(22004.4355, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: tensor(21900.8086, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 loss: tensor(21775.8262, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 loss: tensor(21723.3789, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 loss: tensor(21668.1406, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 loss: tensor(21592.9648, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 loss: tensor(21436.9199, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 loss: tensor(21419.0859, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:22<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 loss: tensor(21362.3887, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 loss: tensor(21262.2793, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss: tensor(21206.2500, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:21<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(21260.9121, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 19585,
     "status": "ok",
     "timestamp": 1639765311334,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "dxe5YEgyPxgW"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1639766225537,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "lBt0CAnBP6KA",
    "outputId": "5a67bf75-9051-44e0-a8dc-d64ab7e9ab11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ballot': ['nominating',\n",
       "  'scatters',\n",
       "  'larceny',\n",
       "  'disenfranchised',\n",
       "  'election',\n",
       "  'two',\n",
       "  'troopers',\n",
       "  'suns',\n",
       "  'boite'],\n",
       " 'cup': ['teaspoonful',\n",
       "  'ketchup',\n",
       "  'worcestershire',\n",
       "  'celery',\n",
       "  'tablespoons',\n",
       "  'chopped',\n",
       "  'chives',\n",
       "  'rub',\n",
       "  'teaspoon'],\n",
       " 'football': ['volleyball',\n",
       "  'letterman',\n",
       "  'mastodons',\n",
       "  'university',\n",
       "  'maladroit',\n",
       "  'facet',\n",
       "  'minn',\n",
       "  'reharmonization',\n",
       "  'rozelle'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'bockwurst',\n",
       "  'knackwurst',\n",
       "  'cervelat',\n",
       "  'bologna',\n",
       "  'lobster',\n",
       "  'pepperoni',\n",
       "  'mischief',\n",
       "  'jerusalem'],\n",
       " 'pitcher': ['overhand',\n",
       "  'gentlemanly',\n",
       "  'ball',\n",
       "  'haydon',\n",
       "  'hefty',\n",
       "  'stop',\n",
       "  'wryly',\n",
       "  'ran',\n",
       "  'away'],\n",
       " 'tablespoon': ['teaspoons',\n",
       "  'worcestershire',\n",
       "  'vinegar',\n",
       "  'chive',\n",
       "  'teaspoon',\n",
       "  'tablespoons',\n",
       "  'horseradish',\n",
       "  'chutney',\n",
       "  'tangy']}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','ballot','mettwurst','pitcher','football','cup']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNlmhSpkoftikmrBJ18vpIr",
   "collapsed_sections": [],
   "name": "Word2Vec - skipGram with hierarchical softmax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
