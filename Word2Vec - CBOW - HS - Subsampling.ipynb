{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1641395298346,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "icJZGDbKDAw2",
    "outputId": "ac5f58a1-7b15-4de5-a34a-749f693e1ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n",
      "{1: [1, 1], 0: [1, 0], 3: [0, 1, 1], 4: [0, 1, 0], 2: [0, 0]}\n",
      "{1: [8, 7], 0: [8, 7], 3: [8, 6, 5], 4: [8, 6, 5], 2: [8, 6]}\n",
      "[8, 7]\n",
      "[8, 7]\n",
      "[8, 6]\n",
      "[8, 6, 5]\n",
      "[8, 6, 5]\n",
      "([[8], [8, 7], [], [6, 5], [6]], [[7], [], [8, 6], [8], [8, 5]])\n"
     ]
    }
   ],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, word_id, frequency):\n",
    "        self.word_id = word_id\n",
    "        self.frequency = frequency\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.father = None\n",
    "        self.Huffman_code = []\n",
    "        self.path = []\n",
    "\n",
    "\n",
    "class HuffmanTree:\n",
    "    def __init__(self, wordid_frequency_dict):\n",
    "        self.word_count = len(wordid_frequency_dict)\n",
    "        self.wordid_code = dict()\n",
    "        self.wordid_path = dict()\n",
    "        self.root = None\n",
    "        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] # Unmerged node list\n",
    "        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] # Store all leaf nodes and intermediate nodes\n",
    "        # Build huffman tree\n",
    "        self.build_tree(unmerge_node_list)\n",
    "        # Generate huffman code\n",
    "        self.generate_huffman_code_and_path()\n",
    "\n",
    "    def merge_node(self, node1, node2):\n",
    "        sum_frequency = node1.frequency + node2.frequency\n",
    "        mid_node_id = len(self.huffman)\n",
    "        father_node = HuffmanNode(mid_node_id, sum_frequency)\n",
    "        if node1.frequency >= node2.frequency:\n",
    "            father_node.left_child = node1\n",
    "            father_node.right_child = node2\n",
    "        else:\n",
    "            father_node.left_child = node2\n",
    "            father_node.right_child = node1\n",
    "        self.huffman.append(father_node)\n",
    "        return father_node\n",
    "\n",
    "    def build_tree(self, node_list):\n",
    "        while len(node_list) > 1:\n",
    "            i1 = 0  # Node with least frequency\n",
    "            i2 = 1  # Node with the second smallest frequency\n",
    "            if node_list[i2].frequency < node_list[i1].frequency:\n",
    "                [i1, i2] = [i2, i1]\n",
    "            for i in range(2, len(node_list)):\n",
    "                if node_list[i].frequency < node_list[i2].frequency:\n",
    "                    i2 = i\n",
    "                    if node_list[i2].frequency < node_list[i1].frequency:\n",
    "                        [i1, i2] = [i2, i1]\n",
    "            father_node = self.merge_node(node_list[i1], node_list[i2])  # Combine the least frequent two nodes\n",
    "            if i1 < i2:\n",
    "                node_list.pop(i2)\n",
    "                node_list.pop(i1)\n",
    "            elif i1 > i2:\n",
    "                node_list.pop(i1)\n",
    "                node_list.pop(i2)\n",
    "            else:\n",
    "                raise RuntimeError('i1 should not be equal to i2')\n",
    "            node_list.insert(0, father_node)  # Insert new node\n",
    "        self.root = node_list[0]\n",
    "\n",
    "    def generate_huffman_code_and_path(self):\n",
    "        stack = [self.root]\n",
    "        while len(stack) > 0:\n",
    "            node = stack.pop()\n",
    "            while node.left_child or node.right_child:\n",
    "                code = node.Huffman_code\n",
    "                path = node.path\n",
    "                node.left_child.Huffman_code = code + [1]\n",
    "                node.right_child.Huffman_code = code + [0]\n",
    "                node.left_child.path = path + [node.word_id]\n",
    "                node.right_child.path = path + [node.word_id]\n",
    "                stack.append(node.right_child)\n",
    "                node = node.left_child\n",
    "            word_id = node.word_id\n",
    "            word_code = node.Huffman_code\n",
    "            word_path = node.path\n",
    "            self.huffman[word_id].Huffman_code = word_code\n",
    "            self.huffman[word_id].path = word_path\n",
    "            # Write the Huffman code and path calculated by the node into the value of the dictionary\n",
    "            self.wordid_code[word_id] = word_code\n",
    "            self.wordid_path[word_id] = word_path\n",
    "\n",
    "    # Get the id of all positive nodes and the id of all negative nodes\n",
    "    def get_all_pos_and_neg_path(self):\n",
    "        positive = []  # Array of positive paths of all words\n",
    "        negative = []  # Array of negative paths for all words\n",
    "        for word_id in range(self.word_count):\n",
    "            pos_id = []\n",
    "            neg_id = []\n",
    "            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n",
    "                if code == 1:\n",
    "                    pos_id.append(self.huffman[word_id].path[i])\n",
    "                else:\n",
    "                    neg_id.append(self.huffman[word_id].path[i])\n",
    "            positive.append(pos_id)\n",
    "            negative.append(neg_id)\n",
    "        return positive, negative\n",
    "\n",
    "\n",
    "def test():\n",
    "    word_frequency = {0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n",
    "    print(word_frequency)\n",
    "    tree = HuffmanTree(word_frequency)\n",
    "    print(tree.wordid_code)\n",
    "    print(tree.wordid_path)\n",
    "    for i in range(len(word_frequency)):\n",
    "        print(tree.huffman[i].path)\n",
    "    print(tree.get_all_pos_and_neg_path())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22312,
     "status": "ok",
     "timestamp": 1641395372034,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "oIVrAob4DdRA",
    "outputId": "906c8631-004b-4339-e147-bea3b58dfdc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Word Count is: 12125\n",
      "Word Count Sum is 55642\n",
      "Sentence Count is: 4538\n",
      "Tree Node is: 24249\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "the fulton grand jury friday an investigation atlantas recent primary election produced evidence irregularities took place\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "jury further termend presentments city executive committee which overall charge election deserves praise thanks of atlanta manner election conducted\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "[([12125, 12125, 1, 3], 0), ([12125, 0, 3, 4], 1), ([0, 1, 4, 6], 3), ([1, 3, 6, 7], 4), ([3, 4, 7, 8], 6), ([4, 6, 8, 10], 7), ([6, 7, 10, 11], 8), ([7, 8, 11, 12], 10), ([8, 10, 12, 13], 11), ([10, 11, 13, 14], 12)]\n",
      "[(['fulton', 'grand'], 'the'), (['the', 'grand', 'jury'], 'fulton'), (['the', 'fulton', 'jury', 'friday'], 'grand'), (['fulton', 'grand', 'friday', 'an'], 'jury'), (['grand', 'jury', 'an', 'investigation'], 'friday'), (['jury', 'friday', 'investigation', 'atlantas'], 'an'), (['friday', 'an', 'atlantas', 'recent'], 'investigation'), (['an', 'investigation', 'recent', 'primary'], 'atlantas'), (['investigation', 'atlantas', 'primary', 'election'], 'recent'), (['atlantas', 'recent', 'election', 'produced'], 'primary')]\n",
      "\n",
      "[24248, 24247]\n",
      "[24245, 24240]\n",
      "[([12125, 12125, 1, 3], 24248), ([12125, 12125, 1, 3], 24247), ([12125, 0, 3, 4], 24248), ([12125, 0, 3, 4], 24247), ([12125, 0, 3, 4], 24245), ([12125, 0, 3, 4], 24129), ([12125, 0, 3, 4], 24029), ([12125, 0, 3, 4], 23510), ([0, 1, 4, 6], 24246), ([0, 1, 4, 6], 24243), ([0, 1, 4, 6], 24225), ([0, 1, 4, 6], 24203), ([0, 1, 4, 6], 23273), ([0, 1, 4, 6], 22533), ([1, 3, 6, 7], 24248), ([1, 3, 6, 7], 24244), ([1, 3, 6, 7], 24209), ([1, 3, 6, 7], 24109), ([1, 3, 6, 7], 23991), ([3, 4, 7, 8], 24248), ([3, 4, 7, 8], 24168), ([3, 4, 7, 8], 24099), ([3, 4, 7, 8], 23972), ([3, 4, 7, 8], 23748), ([3, 4, 7, 8], 23338), ([4, 6, 8, 10], 24246), ([4, 6, 8, 10], 24243), ([4, 6, 8, 10], 24237), ([6, 7, 10, 11], 24248), ([6, 7, 10, 11], 24238), ([6, 7, 10, 11], 24208), ([6, 7, 10, 11], 23984), ([6, 7, 10, 11], 22686), ([6, 7, 10, 11], 21557), ([7, 8, 11, 12], 24196), ([7, 8, 11, 12], 24141), ([7, 8, 11, 12], 23879), ([7, 8, 11, 12], 23013), ([8, 10, 12, 13], 24246), ([8, 10, 12, 13], 24243), ([8, 10, 12, 13], 24237), ([8, 10, 12, 13], 24204), ([8, 10, 12, 13], 24165), ([8, 10, 12, 13], 24098), ([8, 10, 12, 13], 23744), ([8, 10, 12, 13], 23290), ([10, 11, 13, 14], 24246), ([10, 11, 13, 14], 24236), ([10, 11, 13, 14], 24156), ([10, 11, 13, 14], 24077), ([10, 11, 13, 14], 22457)]\n",
      "[([12125, 12125, 1, 3], 24245), ([12125, 12125, 1, 3], 24240), ([12125, 0, 3, 4], 24241), ([12125, 0, 3, 4], 24232), ([12125, 0, 3, 4], 24215), ([12125, 0, 3, 4], 24186), ([12125, 0, 3, 4], 23841), ([12125, 0, 3, 4], 22894), ([12125, 0, 3, 4], 21937), ([0, 1, 4, 6], 24248), ([0, 1, 4, 6], 24237), ([0, 1, 4, 6], 24163), ([0, 1, 4, 6], 24089), ([0, 1, 4, 6], 23954), ([0, 1, 4, 6], 23712), ([1, 3, 6, 7], 24247), ([1, 3, 6, 7], 24239), ([1, 3, 6, 7], 24229), ([1, 3, 6, 7], 24175), ([1, 3, 6, 7], 23774), ([1, 3, 6, 7], 23385), ([3, 4, 7, 8], 24247), ([3, 4, 7, 8], 24244), ([3, 4, 7, 8], 24238), ([3, 4, 7, 8], 24227), ([3, 4, 7, 8], 24206), ([4, 6, 8, 10], 24248), ([4, 6, 8, 10], 24226), ([4, 6, 8, 10], 24204), ([4, 6, 8, 10], 24164), ([4, 6, 8, 10], 24091), ([6, 7, 10, 11], 24247), ([6, 7, 10, 11], 24244), ([6, 7, 10, 11], 24228), ([6, 7, 10, 11], 24173), ([6, 7, 10, 11], 24106), ([6, 7, 10, 11], 23767), ([6, 7, 10, 11], 23363), ([7, 8, 11, 12], 24248), ([7, 8, 11, 12], 24246), ([7, 8, 11, 12], 24242), ([7, 8, 11, 12], 24234), ([7, 8, 11, 12], 24218), ([7, 8, 11, 12], 24064), ([7, 8, 11, 12], 23637), ([7, 8, 11, 12], 22360), ([7, 8, 11, 12], 20606), ([7, 8, 11, 12], 19413), ([8, 10, 12, 13], 24248), ([8, 10, 12, 13], 24226), ([8, 10, 12, 13], 23960), ([8, 10, 12, 13], 22635), ([10, 11, 13, 14], 24248), ([10, 11, 13, 14], 24243), ([10, 11, 13, 14], 24224), ([10, 11, 13, 14], 24200), ([10, 11, 13, 14], 23933), ([10, 11, 13, 14], 23674), ([10, 11, 13, 14], 23216)]\n",
      "[([11, 12, 14, 16], 13), ([12, 13, 16, 19], 14), ([13, 14, 19, 20], 16), ([14, 16, 20, 21], 19), ([16, 19, 21, 12125], 20), ([19, 20, 12125, 12125], 21), ([12125, 12125, 22, 24], 4), ([12125, 4, 24, 25], 22), ([4, 22, 25, 26], 24), ([22, 24, 26, 27], 25)]\n",
      "[(['recent', 'primary', 'produced', 'evidence'], 'election'), (['primary', 'election', 'evidence', 'irregularities'], 'produced'), (['election', 'produced', 'irregularities', 'took'], 'evidence'), (['produced', 'evidence', 'took', 'place'], 'irregularities'), (['evidence', 'irregularities', 'place'], 'took'), (['irregularities', 'took'], 'place'), (['further', 'termend'], 'jury'), (['jury', 'termend', 'presentments'], 'further'), (['jury', 'further', 'presentments', 'city'], 'termend'), (['further', 'termend', 'city', 'executive'], 'presentments')]\n",
      "minnesota\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences, sample):\n",
    "        self.norm_sentences = []\n",
    "        self.sample = sample\n",
    "        self.counter = 0\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  # Number of words (repeated words only count as 1)\n",
    "        self.word_count_sum = 0  # Total number of words (the number of repeated words also accumulates)\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict(sentences)  # Initialize the dictionary\n",
    "        self.subsampling()\n",
    "        self.huffman_tree = HuffmanTree(self.wordId_frequency_dict)  # Hoffman Tree\n",
    "        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "        print('Tree Node is:', len(self.huffman_tree.huffman))\n",
    "\n",
    "\n",
    "    def subsampling(self):\n",
    "        \n",
    "        if self.sample > 0:\n",
    "            self.word_count_sum = 0\n",
    "            self.sentence_count = 0\n",
    "\n",
    "            frequency = np.array(list(self.wordId_frequency_dict.values()))\n",
    "            z = frequency / np.sum(frequency)\n",
    "            p = (np.sqrt(z / self.sample) + 1) * (self.sample / z)\n",
    "\n",
    "            new_norm_sentences = []\n",
    "            for word_list in self.norm_sentences:\n",
    "              word_list = [word for word in word_list if p[self.word2id_dict[word]] > random.random()]\n",
    "              if len(word_list) >= 2:\n",
    "                self.sentence_count += 1\n",
    "                self.word_count_sum += len(word_list)\n",
    "                new_norm_sentences.append(word_list)\n",
    "\n",
    "            self.norm_sentences = new_norm_sentences\n",
    "\n",
    "    def normalize(self, word_list):\n",
    "      sentence = \" \".join(word for word in word_list)\n",
    "      sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "      sentence = sentence.lower()\n",
    "      sentence = re.sub(' +', ' ', sentence)\n",
    "      sentence = sentence.strip()\n",
    "      norm_word_list = sentence.split(' ')\n",
    "      if self.sample <= 0:\n",
    "          stop_words = nltk.corpus.stopwords.words('english')\n",
    "          norm_word_list_with_out_stop_words = [word for word in norm_word_list if word not in stop_words]\n",
    "          norm_word_list = norm_word_list_with_out_stop_words\n",
    "\n",
    "      return norm_word_list\n",
    "\n",
    "    def _init_dict(self,sentences):\n",
    "        word_freq = dict()\n",
    "        for word_list in sentences:\n",
    "            word_list = self.normalize(word_list)\n",
    "            if(len(word_list) < 2):\n",
    "                continue\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "            self.norm_sentences.append(word_list)\n",
    "        word_id = 0\n",
    "        # Initialize word2id_dict, id2word_dict, wordId_frequency_dict dictionary\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def generate_context_word_pairs(self, window_size):\n",
    "      self.counter += 1\n",
    "      if not self.norm_sentences[20*(self.counter-1):20*self.counter]:\n",
    "        self.counter = 1\n",
    "        self.word_pairs_queue.clear()\n",
    "      sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.norm_sentences[20*(self.counter-1):20*self.counter]]\n",
    "      context_length = window_size*2\n",
    "      for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, word in enumerate(words):         \n",
    "              start = index - window_size\n",
    "              end = index + window_size + 1\n",
    "\n",
    "              context_words = []\n",
    "              for i in range(start, end):\n",
    "                  if 0 <= i < sentence_length and i != index:\n",
    "                    context_words.append(words[i])\n",
    "                  elif i < 0 or i >= sentence_length:\n",
    "                    context_words.append(self.word_count)\n",
    "\n",
    "              self.word_pairs_queue.append((context_words,word))\n",
    "\n",
    "    # Get the positive sample pair (Xw,w) of the mini-batch size. Xw is the context id array, and w is the center word id. The context step size is window_size, ie 2c = 2*window_size\n",
    "    def get_batch_pairs(self, batch_size, window_size):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_context_word_pairs(window_size)\n",
    "\n",
    "        result_pairs = []  # Returns the positive sample pair of mini-batch size\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "    def get_pairs(self, pos_pairs):\n",
    "        neg_word_pair = []\n",
    "        pos_word_pair = []\n",
    "        for pair in pos_pairs:\n",
    "            pos_word_pair += zip([pair[0]] * len(self.huffman_pos_path[pair[1]]), self.huffman_pos_path[pair[1]])\n",
    "            neg_word_pair += zip([pair[0]] * len(self.huffman_neg_path[pair[1]]), self.huffman_neg_path[pair[1]])\n",
    "        return pos_word_pair, neg_word_pair\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "# Test all methods\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])\n",
    "    test_data = InputData(sentences,0.0002)\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[1]))\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2)\n",
    "    print(sentences[0])\n",
    "    print(sentences[1])\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append(([test_data.id2word_dict[i] for i in pair[0] if i != test_data.word_count], test_data.id2word_dict[pair[1]]))\n",
    "    print(pos_word_pairs)\n",
    "    print('')\n",
    "    print(test_data.huffman_pos_path[0])\n",
    "    print(test_data.huffman_neg_path[0])\n",
    "    pos, neg = test_data.get_pairs(pos_pairs)\n",
    "    print(pos)\n",
    "    print(neg)\n",
    "\n",
    "    pos_word_pairs = []\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2)\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append(([test_data.id2word_dict[i] for i in pair[0] if i != test_data.word_count], test_data.id2word_dict[pair[1]]))\n",
    "    print(pos_pairs)\n",
    "    print(pos_word_pairs)\n",
    "\n",
    "    print(test_data.id2word_dict[4846])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6793,
     "status": "ok",
     "timestamp": 1641395400508,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "jrBu1I1KEpDB",
    "outputId": "0ea163b5-b7c3-4f23-930b-ae3a46da4ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(2*self.emb_size-1, self.emb_dimension, sparse=True)  # Define the embedded dictionary for the input word\n",
    "        self.w_embeddings = nn.Embedding(2*self.emb_size-1, self.emb_dimension, sparse=True)  # Define the embedded dictionary for the output word\n",
    "        self._init_embedding()  # initialization\n",
    "\n",
    "    def _init_embedding(self):\n",
    "        int_range = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-int_range, int_range) #work\n",
    "        self.w_embeddings.weight.data.uniform_(-0, 0) #work\n",
    "\n",
    "    def compute_context_matrix(self, u):\n",
    "        pos_u_emb = self.u_embeddings(torch.LongTensor(u))\n",
    "        pos_u_emb = torch.mean(pos_u_emb, 1, True)\n",
    "        pos_u_emb = pos_u_emb.squeeze()\n",
    "\n",
    "        return pos_u_emb\n",
    "\n",
    "    def forward(self, pos_u, pos_w, neg_u, neg_w):\n",
    "        pos_u_emb = self.compute_context_matrix(pos_u)\n",
    "        pos_w_emb = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "        neg_u_emb = self.compute_context_matrix(neg_u)\n",
    "        neg_w_emb = self.w_embeddings(torch.LongTensor(neg_w))\n",
    "\n",
    "        # Gradient descent（ result *（-1） Can become a loss function -> Gradient descent using torch）\n",
    "        score_1 = torch.mul(pos_u_emb, pos_w_emb).squeeze()  # Xw.T * θu\n",
    "        score_2 = torch.sum(score_1, dim=1)\n",
    "        score_3 = F.logsigmoid(score_2)\n",
    "        neg_score_1 = torch.mul(neg_u_emb, neg_w_emb).squeeze()  # Xw.T * θw\n",
    "        neg_score_2 = torch.sum(neg_score_1, dim=1)\n",
    "        neg_score_3 = F.logsigmoid(-1 * neg_score_2)\n",
    "        # L = log sigmoid (Xw.T * θw) + logsigmoid (-Xw.T * θw)\n",
    "        loss = torch.sum(score_3) + torch.sum(neg_score_3)\n",
    "        return -1 * loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.u_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = CBOWModel(100, 10)\n",
    "\n",
    "    pos_u = [[9, 1, 2, 3],[0, 1, 2, 3]]\n",
    "    pos_w = [50, 70]\n",
    "    neg_u = [[9, 1, 2, 3],[0, 1, 2, 3]]\n",
    "    neg_w = [30, 42]\n",
    "    model.forward(pos_u, pos_w, neg_u, neg_w)\n",
    "    distance_matrix = model.distance_matrix(5)\n",
    "    print(distance_matrix.shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1641395419738,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "RASy935IDyTm"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 2 \n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.01  # Learning rate\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self,sentences,sample):\n",
    "        self.data = InputData(sentences,sample)\n",
    "        self.model = CBOWModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        lambda1 = lambda epoch: 0.99 ** epoch\n",
    "        self.scheduler = LambdaLR(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"CBOW Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,21):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
    "                pos_pairs, neg_pairs = self.data.get_pairs(pos_pairs)\n",
    "\n",
    "                pos_u = [pair[0] for pair in pos_pairs]\n",
    "                pos_v = [int(pair[1]) for pair in pos_pairs]\n",
    "                neg_u = [pair[0] for pair in neg_pairs]\n",
    "                neg_v = [int(pair[1]) for pair in neg_pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_u,neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n",
    "\n",
    "#def run():\n",
    "    #sentences = brown.sents(categories=['news','reviews','humor','hobbies'])\n",
    "    #w2v = Word2Vec(sentences)\n",
    "    #w2v.train()\n",
    "    \n",
    "    #result = w2v.similar_words(['government','church','children','car','tax','food','election'])\n",
    "\n",
    "    #print(result)            \n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104804,
     "status": "ok",
     "timestamp": 1641395529520,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "ndkrLkLhynWq",
    "outputId": "1a550d9b-27ec-4b39-cbb3-97c70fa4eb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count is: 24758\n",
      "Word Count Sum is 196583\n",
      "Sentence Count is: 17270\n",
      "Tree Node is: 49515\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "SAMPLE = 0.0002 # use subsampling\n",
    "w2v = Word2Vec(sentences,SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477841,
     "status": "ok",
     "timestamp": 1641406140397,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "V_aNo2Oefp3n",
    "outputId": "449e4dec-da23-476f-9150-54b87b4fea7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Training......\n",
      "pairs_count 196583\n",
      "batch_count 196.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:24<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(4780.1309, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(4764.9468, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(4747.4893, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(4746.4795, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(4726.6245, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(4706.8091, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(4696.3066, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(4692.0308, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(4673.9199, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(4662.0986, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:25<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(4644.7568, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(4636.8027, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(4622.9019, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(4607.2778, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(4596.1562, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(4584.8252, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(4574.9326, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(4560.4326, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(4550.4282, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:23<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(4540.1343, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 17367,
     "status": "ok",
     "timestamp": 1641406162188,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "0gt5KV8kzIz6"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1641406820599,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "On08un3-zKXk",
    "outputId": "e5fb379b-db91-4394-8916-7a793bcd4ed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'democratic': ['republican',\n",
       "  'welton',\n",
       "  'sukarno',\n",
       "  'philharmonique',\n",
       "  'couve',\n",
       "  'nato',\n",
       "  'dictators',\n",
       "  'macdonald',\n",
       "  'idol'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'foam',\n",
       "  'geddes',\n",
       "  'macgregors',\n",
       "  'cigar',\n",
       "  'loews',\n",
       "  'markers',\n",
       "  'awnings',\n",
       "  'simpleminded'],\n",
       " 'sauce': ['basics',\n",
       "  'cervelat',\n",
       "  'celery',\n",
       "  'tomato',\n",
       "  'listenersupported',\n",
       "  'lids',\n",
       "  'flaky',\n",
       "  'crimson',\n",
       "  'sheathing'],\n",
       " 'tax': ['expenditures',\n",
       "  'grants',\n",
       "  'january',\n",
       "  'sum',\n",
       "  'granted',\n",
       "  'rights',\n",
       "  'directed',\n",
       "  'contract',\n",
       "  'surveyed']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tax','sauce', 'democratic','mettwurst']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMI3swhLf8knGqu39maZxp5",
   "collapsed_sections": [],
   "name": "Word2Vec - CBOW with hierarchical softmax and subsampling.ipynb",
   "provenance": [
    {
     "file_id": "1afYzQgbyobjAXqsJnXH0q_pFhKp4ARUt",
     "timestamp": 1639935466222
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
