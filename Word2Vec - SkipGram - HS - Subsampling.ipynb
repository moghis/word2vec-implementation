{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1641407578381,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "SOT0vcq8GhLw",
    "outputId": "fb1cb180-750f-41d1-b16e-626ca761ca8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n",
      "{1: [1, 1], 0: [1, 0], 3: [0, 1, 1], 4: [0, 1, 0], 2: [0, 0]}\n",
      "{1: [8, 7], 0: [8, 7], 3: [8, 6, 5], 4: [8, 6, 5], 2: [8, 6]}\n",
      "[8, 7]\n",
      "[8, 7]\n",
      "[8, 6]\n",
      "[8, 6, 5]\n",
      "[8, 6, 5]\n",
      "([[8], [8, 7], [], [6, 5], [6]], [[7], [], [8, 6], [8], [8, 5]])\n"
     ]
    }
   ],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, word_id, frequency):\n",
    "        self.word_id = word_id\n",
    "        self.frequency = frequency\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.father = None\n",
    "        self.Huffman_code = []\n",
    "        self.path = []\n",
    "\n",
    "\n",
    "class HuffmanTree:\n",
    "    def __init__(self, wordid_frequency_dict):\n",
    "        self.word_count = len(wordid_frequency_dict)\n",
    "        self.wordid_code = dict()\n",
    "        self.wordid_path = dict()\n",
    "        self.root = None\n",
    "        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()]\n",
    "        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()]\n",
    "        self.build_tree(unmerge_node_list)\n",
    "        self.generate_huffman_code_and_path()\n",
    "\n",
    "    def merge_node(self, node1, node2):\n",
    "        sum_frequency = node1.frequency + node2.frequency\n",
    "        mid_node_id = len(self.huffman)\n",
    "        father_node = HuffmanNode(mid_node_id, sum_frequency)\n",
    "        if node1.frequency >= node2.frequency:\n",
    "            father_node.left_child = node1\n",
    "            father_node.right_child = node2\n",
    "        else:\n",
    "            father_node.left_child = node2\n",
    "            father_node.right_child = node1\n",
    "        self.huffman.append(father_node)\n",
    "        return father_node\n",
    "\n",
    "    def build_tree(self, node_list):\n",
    "        while len(node_list) > 1:\n",
    "            i1 = 0\n",
    "            i2 = 1\n",
    "            if node_list[i2].frequency < node_list[i1].frequency:\n",
    "                [i1, i2] = [i2, i1]\n",
    "            for i in range(2, len(node_list)):\n",
    "                if node_list[i].frequency < node_list[i2].frequency:\n",
    "                    i2 = i\n",
    "                    if node_list[i2].frequency < node_list[i1].frequency:\n",
    "                        [i1, i2] = [i2, i1]\n",
    "            father_node = self.merge_node(node_list[i1], node_list[i2])\n",
    "            if i1 < i2:\n",
    "                node_list.pop(i2)\n",
    "                node_list.pop(i1)\n",
    "            elif i1 > i2:\n",
    "                node_list.pop(i1)\n",
    "                node_list.pop(i2)\n",
    "            else:\n",
    "                raise RuntimeError('i1 should not be equal to i2')\n",
    "            node_list.insert(0, father_node)\n",
    "        self.root = node_list[0]\n",
    "\n",
    "    def generate_huffman_code_and_path(self):\n",
    "        stack = [self.root]\n",
    "        while len(stack) > 0:\n",
    "            node = stack.pop()\n",
    "            while node.left_child or node.right_child:\n",
    "                code = node.Huffman_code\n",
    "                path = node.path\n",
    "                node.left_child.Huffman_code = code + [1]\n",
    "                node.right_child.Huffman_code = code + [0]\n",
    "                node.left_child.path = path + [node.word_id]\n",
    "                node.right_child.path = path + [node.word_id]\n",
    "                stack.append(node.right_child)\n",
    "                node = node.left_child\n",
    "            word_id = node.word_id\n",
    "            word_code = node.Huffman_code\n",
    "            word_path = node.path\n",
    "            self.huffman[word_id].Huffman_code = word_code\n",
    "            self.huffman[word_id].path = word_path\n",
    "            self.wordid_code[word_id] = word_code\n",
    "            self.wordid_path[word_id] = word_path\n",
    "\n",
    "    def get_all_pos_and_neg_path(self):\n",
    "        positive = []\n",
    "        negative = []\n",
    "        for word_id in range(self.word_count):\n",
    "            pos_id = []\n",
    "            neg_id = []\n",
    "            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n",
    "                if code == 1:\n",
    "                    pos_id.append(self.huffman[word_id].path[i])\n",
    "                else:\n",
    "                    neg_id.append(self.huffman[word_id].path[i])\n",
    "            positive.append(pos_id)\n",
    "            negative.append(neg_id)\n",
    "        return positive, negative\n",
    "\n",
    "\n",
    "def test():\n",
    "    word_frequency = {0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n",
    "    print(word_frequency)\n",
    "    tree = HuffmanTree(word_frequency)\n",
    "    print(tree.wordid_code)\n",
    "    print(tree.wordid_path)\n",
    "    for i in range(len(word_frequency)):\n",
    "        print(tree.huffman[i].path)\n",
    "    print(tree.get_all_pos_and_neg_path())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18688,
     "status": "ok",
     "timestamp": 1641407602591,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "CEaJ1tAeRWYh",
    "outputId": "0710e24d-9362-43fa-e443-6abc2064eb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Word Count is: 11992\n",
      "Word Count Sum is 50006\n",
      "Sentence Count is: 4532\n",
      "Tree Node is: 23983\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "fulton county grand jury said friday investigation atlantas recent primary election produced evidence irregularities took place\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "jury said termend presentments city executive committee overall charge election deserves praise thanks city atlanta manner election conducted\n",
      "[(0, [1, 2]), (1, [0, 2, 3]), (2, [0, 1, 3, 4]), (3, [1, 2, 4, 5]), (4, [2, 3, 5, 6]), (5, [3, 4, 6, 7]), (6, [4, 5, 7, 8]), (7, [5, 6, 8, 9]), (8, [6, 7, 9, 10]), (9, [7, 8, 10, 11])]\n",
      "[('fulton', ['county', 'grand']), ('county', ['fulton', 'grand', 'jury']), ('grand', ['fulton', 'county', 'jury', 'said']), ('jury', ['county', 'grand', 'said', 'friday']), ('said', ['grand', 'jury', 'friday', 'investigation']), ('friday', ['jury', 'said', 'investigation', 'atlantas']), ('investigation', ['said', 'friday', 'atlantas', 'recent']), ('atlantas', ['friday', 'investigation', 'recent', 'primary']), ('recent', ['investigation', 'atlantas', 'primary', 'election']), ('primary', ['atlantas', 'recent', 'election', 'produced'])]\n",
      "[23982, 23978, 23973, 23818, 23342, 21791]\n",
      "[23981, 23963, 23942, 23899, 23650, 22739]\n",
      "[23982, 23981, 23824, 23668, 23367]\n",
      "[23979, 23974, 23964, 23944, 23904]\n",
      "[23976, 23969, 23533, 23113]\n",
      "[23982, 23980, 23955, 23926, 23868, 23754, 22383]\n",
      "[(0, 23982), (0, 23981), (0, 23824), (0, 23668), (0, 23367), (0, 23976), (0, 23969), (0, 23533), (0, 23113), (1, 23982), (1, 23978), (1, 23973), (1, 23818), (1, 23342), (1, 21791), (1, 23976), (1, 23969), (1, 23533), (1, 23113), (1, 23980), (1, 23977), (1, 23971), (1, 23785), (1, 23591), (2, 23982), (2, 23978), (2, 23973), (2, 23818), (2, 23342), (2, 21791), (2, 23982), (2, 23981), (2, 23824), (2, 23668), (2, 23367), (2, 23980), (2, 23977), (2, 23971), (2, 23785), (2, 23591), (2, 23982), (2, 23972), (2, 23961), (2, 23894), (3, 23982), (3, 23981), (3, 23824), (3, 23668), (3, 23367), (3, 23976), (3, 23969), (3, 23533), (3, 23113), (3, 23982), (3, 23972), (3, 23961), (3, 23894), (3, 23980), (3, 23970), (3, 23930), (3, 23877), (3, 23772), (3, 23177), (4, 23976), (4, 23969), (4, 23533), (4, 23113), (4, 23980), (4, 23977), (4, 23971), (4, 23785), (4, 23591), (4, 23980), (4, 23970), (4, 23930), (4, 23877), (4, 23772), (4, 23177), (4, 23980), (4, 23977), (4, 23932), (4, 23882), (4, 23584), (4, 23200), (4, 21412), (5, 23980), (5, 23977), (5, 23971), (5, 23785), (5, 23591), (5, 23982), (5, 23972), (5, 23961), (5, 23894), (5, 23980), (5, 23977), (5, 23932), (5, 23882), (5, 23584), (5, 23200), (5, 21412), (5, 23982), (5, 23981), (5, 23979), (5, 23965), (5, 22856), (5, 20465), (5, 19274), (6, 23982), (6, 23972), (6, 23961), (6, 23894), (6, 23980), (6, 23970), (6, 23930), (6, 23877), (6, 23772), (6, 23177), (6, 23982), (6, 23981), (6, 23979), (6, 23965), (6, 22856), (6, 20465), (6, 19274), (6, 23976), (6, 23969), (6, 23955), (6, 23927), (6, 23876), (7, 23980), (7, 23970), (7, 23930), (7, 23877), (7, 23772), (7, 23177), (7, 23980), (7, 23977), (7, 23932), (7, 23882), (7, 23584), (7, 23200), (7, 21412), (7, 23976), (7, 23969), (7, 23955), (7, 23927), (7, 23876), (7, 23952), (7, 23921), (7, 23859), (7, 23737), (8, 23980), (8, 23977), (8, 23932), (8, 23882), (8, 23584), (8, 23200), (8, 21412), (8, 23982), (8, 23981), (8, 23979), (8, 23965), (8, 22856), (8, 20465), (8, 19274), (8, 23952), (8, 23921), (8, 23859), (8, 23737), (8, 23980), (8, 23970), (8, 23930), (8, 23877), (8, 23772), (9, 23982), (9, 23981), (9, 23979), (9, 23965), (9, 22856), (9, 20465), (9, 19274), (9, 23976), (9, 23969), (9, 23955), (9, 23927), (9, 23876), (9, 23980), (9, 23970), (9, 23930), (9, 23877), (9, 23772), (9, 23982), (9, 23960), (9, 23936), (9, 23890), (9, 23793), (9, 22641), (9, 20044)]\n",
      "[(0, 23979), (0, 23974), (0, 23964), (0, 23944), (0, 23904), (0, 23982), (0, 23980), (0, 23955), (0, 23926), (0, 23868), (0, 23754), (0, 22383), (1, 23981), (1, 23963), (1, 23942), (1, 23899), (1, 23650), (1, 22739), (1, 23982), (1, 23980), (1, 23955), (1, 23926), (1, 23868), (1, 23754), (1, 22383), (1, 23982), (1, 23959), (1, 23934), (1, 23884), (1, 23221), (2, 23981), (2, 23963), (2, 23942), (2, 23899), (2, 23650), (2, 22739), (2, 23979), (2, 23974), (2, 23964), (2, 23944), (2, 23904), (2, 23982), (2, 23959), (2, 23934), (2, 23884), (2, 23221), (2, 23981), (2, 23978), (2, 23939), (3, 23979), (3, 23974), (3, 23964), (3, 23944), (3, 23904), (3, 23982), (3, 23980), (3, 23955), (3, 23926), (3, 23868), (3, 23754), (3, 22383), (3, 23981), (3, 23978), (3, 23939), (3, 23982), (3, 23977), (3, 23957), (3, 23569), (4, 23982), (4, 23980), (4, 23955), (4, 23926), (4, 23868), (4, 23754), (4, 22383), (4, 23982), (4, 23959), (4, 23934), (4, 23884), (4, 23221), (4, 23982), (4, 23977), (4, 23957), (4, 23569), (4, 23982), (4, 23971), (4, 23958), (4, 23779), (4, 22534), (5, 23982), (5, 23959), (5, 23934), (5, 23884), (5, 23221), (5, 23981), (5, 23978), (5, 23939), (5, 23982), (5, 23971), (5, 23958), (5, 23779), (5, 22534), (5, 23975), (5, 23950), (5, 23908), (5, 23850), (5, 23683), (5, 23465), (5, 22211), (6, 23981), (6, 23978), (6, 23939), (6, 23982), (6, 23977), (6, 23957), (6, 23569), (6, 23975), (6, 23950), (6, 23908), (6, 23850), (6, 23683), (6, 23465), (6, 22211), (6, 23982), (6, 23980), (6, 23759), (6, 23563), (6, 23128), (6, 22483), (7, 23982), (7, 23977), (7, 23957), (7, 23569), (7, 23982), (7, 23971), (7, 23958), (7, 23779), (7, 22534), (7, 23982), (7, 23980), (7, 23759), (7, 23563), (7, 23128), (7, 22483), (7, 23982), (7, 23980), (7, 23976), (7, 23968), (7, 23499), (7, 23057), (7, 22307), (8, 23982), (8, 23971), (8, 23958), (8, 23779), (8, 22534), (8, 23975), (8, 23950), (8, 23908), (8, 23850), (8, 23683), (8, 23465), (8, 22211), (8, 23982), (8, 23980), (8, 23976), (8, 23968), (8, 23499), (8, 23057), (8, 22307), (8, 23982), (8, 23977), (8, 23957), (8, 23569), (8, 23177), (9, 23975), (9, 23950), (9, 23908), (9, 23850), (9, 23683), (9, 23465), (9, 22211), (9, 23982), (9, 23980), (9, 23759), (9, 23563), (9, 23128), (9, 22483), (9, 23982), (9, 23977), (9, 23957), (9, 23569), (9, 23177), (9, 23981), (9, 23978), (9, 23972), (9, 23617), (9, 23250), (9, 21539)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences, sample):\n",
    "        self.norm_sentences = []\n",
    "        self.counter = 0\n",
    "        self.sample = sample\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  # Number of words\n",
    "        self.word_count_sum = 0  # Total number of words\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict(sentences)  # Initialize the dictionary\n",
    "        self.subsampling()\n",
    "        self.huffman_tree = HuffmanTree(self.wordId_frequency_dict)  # Hoffman Tree\n",
    "        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "        print('Tree Node is:', len(self.huffman_tree.huffman))\n",
    "\n",
    "    def subsampling(self):\n",
    "        \n",
    "        if self.sample > 0:\n",
    "            self.word_count_sum = 0\n",
    "            self.sentence_count = 0\n",
    "          \n",
    "            frequency = np.array(list(self.wordId_frequency_dict.values()))\n",
    "            z = frequency / np.sum(frequency)\n",
    "            p = (np.sqrt(z / self.sample) + 1) * (self.sample / z)\n",
    "\n",
    "            new_norm_sentences = []\n",
    "            for word_list in self.norm_sentences:\n",
    "              word_list = [word for word in word_list if p[self.word2id_dict[word]] > random.random()]\n",
    "              if len(word_list) >= 2:\n",
    "                self.sentence_count += 1\n",
    "                self.word_count_sum += len(word_list)\n",
    "                new_norm_sentences.append(word_list)\n",
    "\n",
    "            self.norm_sentences = new_norm_sentences\n",
    "\n",
    "    def normalize(self, word_list):\n",
    "      sentence = \" \".join(word for word in word_list)\n",
    "      sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "      sentence = sentence.lower()\n",
    "      sentence = re.sub(' +', ' ', sentence)\n",
    "      sentence = sentence.strip()\n",
    "      norm_word_list = sentence.split(' ')\n",
    "      if self.sample <= 0:\n",
    "          stop_words = nltk.corpus.stopwords.words('english')\n",
    "          norm_word_list_with_out_stop_words = [word for word in norm_word_list if word not in stop_words]\n",
    "          norm_word_list = norm_word_list_with_out_stop_words\n",
    "\n",
    "      return norm_word_list\n",
    "\n",
    "    def _init_dict(self,sentences):\n",
    "        word_freq = dict()\n",
    "        for word_list in sentences:\n",
    "            word_list = self.normalize(word_list)\n",
    "            if(len(word_list) < 2):\n",
    "              continue\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "            self.norm_sentences.append(word_list)\n",
    "\n",
    "        word_id = 0\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def generate_center_context_pairs(self, window_size):\n",
    "        self.counter += 1\n",
    "        if not self.norm_sentences[20*(self.counter-1):20*self.counter]:\n",
    "            self.counter = 1\n",
    "            self.word_pairs_queue.clear()\n",
    "        sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.norm_sentences[20*(self.counter-1):20*self.counter]]\n",
    "        for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, center_word in enumerate(words):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words = []\n",
    "            for index_2 in range(start,end):\n",
    "              if 0 <= index_2 < sentence_length and index_2 != index:\n",
    "                context_words.append(words[index_2])\n",
    "\n",
    "            self.word_pairs_queue.append((center_word, context_words))           \n",
    "\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_center_context_pairs(window_size)              \n",
    "              \n",
    "        result_pairs = []\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    def get_pos_neg_pairs(self, pairs):\n",
    "        neg_word_pair = []\n",
    "        pos_word_pair = []\n",
    "        for pair in pairs:\n",
    "            for context_word in pair[1]:\n",
    "              pos_word_pair += zip([pair[0]] * len(self.huffman_pos_path[context_word]), self.huffman_pos_path[context_word])\n",
    "              neg_word_pair += zip([pair[0]] * len(self.huffman_neg_path[context_word]), self.huffman_neg_path[context_word])\n",
    "        return pos_word_pair, neg_word_pair\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])\n",
    "    test_data = InputData(sentences,0)\n",
    "\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[1]))\n",
    "\n",
    "    \n",
    "    batch_pairs = test_data.get_batch_pairs(10, 2)\n",
    "    pos_pairs, neg_pairs = test_data.get_pos_neg_pairs(batch_pairs)\n",
    "    batch_word_pairs = []\n",
    "    for pair in batch_pairs:\n",
    "        batch_word_pairs.append((test_data.id2word_dict[pair[0]] , [test_data.id2word_dict[i] for i in pair[1]]))\n",
    "    print(batch_pairs)\n",
    "    print(batch_word_pairs)\n",
    "    \n",
    "    print(test_data.huffman_pos_path[0])\n",
    "    print(test_data.huffman_neg_path[0])\n",
    "    print(test_data.huffman_pos_path[1])\n",
    "    print(test_data.huffman_neg_path[1])\n",
    "    print(test_data.huffman_pos_path[2])\n",
    "    print(test_data.huffman_neg_path[2])\n",
    "\n",
    "    print(pos_pairs)\n",
    "    print(neg_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6426,
     "status": "ok",
     "timestamp": 1641407620285,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "vIC4eGjs8SZ9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(2*emb_size-1, emb_dimension, sparse=True)\n",
    "        self.w_embeddings = nn.Embedding(2*emb_size-1, emb_dimension, sparse=True)\n",
    "        self._init_emb()\n",
    "\n",
    "    def _init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.w_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_u, pos_w, neg_u, neg_w):\n",
    "        pos_u_emb = self.u_embeddings(torch.LongTensor(pos_u))\n",
    "        pos_w_emb = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "\n",
    "        neg_u_emb = self.u_embeddings(torch.LongTensor(neg_u))\n",
    "        neg_w_emb = self.w_embeddings(torch.LongTensor(neg_w))\n",
    "\n",
    "        score = torch.mul(pos_u_emb, pos_w_emb)\n",
    "        score = torch.sum(score, dim=1).squeeze()\n",
    "        score = F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.mul(neg_u_emb, neg_w_emb)\n",
    "        neg_score = torch.sum(neg_score, dim=1).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "\n",
    "        loss = -1 * (torch.sum(score) + torch.sum(neg_score)) \n",
    "        return loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.u_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = SkipGramModel(100, 10)\n",
    "    id2word = dict()\n",
    "    for i in range(100):\n",
    "        id2word[i] = str(i)\n",
    "    pos_u = [0, 0, 1, 1, 1]\n",
    "    pos_w = [102, 134, 173, 183, 148]\n",
    "    neg_u = [0, 0, 0, 1, 1]\n",
    "    neg_w = [123, 166, 192, 111, 177]\n",
    "    model.forward(pos_u, pos_w, neg_u, neg_w)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1641407650582,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "OKt0clpZGPtP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.01  # Learning rate\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self,sentences, sample):\n",
    "        self.data = InputData(sentences, sample)\n",
    "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        # lambda1 = lambda epoch: 0.99 ** epoch\n",
    "        # self.scheduler = SkipGramModel(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"CBOW Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,21):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
    "                pos_pairs, neg_pairs = self.data.get_pos_neg_pairs(pairs)\n",
    "\n",
    "                pos_u = [int(pair[0]) for pair in pos_pairs]\n",
    "                pos_w = [int(pair[1]) for pair in pos_pairs]\n",
    "                neg_u = [int(pair[0]) for pair in neg_pairs]\n",
    "                neg_w = [int(pair[1]) for pair in neg_pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u, pos_w, neg_u, neg_w)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "            # self.scheduler.step()\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96105,
     "status": "ok",
     "timestamp": 1641407749140,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "29w2zJngIMRu",
    "outputId": "7883352a-80e7-481d-d43d-3945d6091f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count is: 24758\n",
      "Word Count Sum is 196644\n",
      "Sentence Count is: 17273\n",
      "Tree Node is: 49515\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "SAMPLE = 0.0002 # use subsampling\n",
    "w2v = Word2Vec(sentences, SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564371,
     "status": "ok",
     "timestamp": 1641412328551,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "vF7oq1M-IN3N",
    "outputId": "cf74d132-fbf9-42a8-dea6-c857eff49173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Training......\n",
      "pairs_count 196644\n",
      "batch_count 196.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:27<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(22043.5820, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:27<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(22044.9316, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(21938.1523, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(22065.8691, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(21965.2090, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(21953.4473, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(21910.7422, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(21833.3828, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(21868.4648, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(21768.5430, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:27<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(21794.1387, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(21757.6309, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(21736.4082, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(21688.6406, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(21723.1680, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(21674.1777, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(21604.7559, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(21616.8223, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(21614.5586, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:29<00:00,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(21657.3828, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 14643,
     "status": "ok",
     "timestamp": 1641412535310,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "dxe5YEgyPxgW"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1641417867303,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "lBt0CAnBP6KA",
    "outputId": "8e639e7a-18fc-40b3-8622-97fecf904555"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cup': ['ketchup',\n",
       "  'tablespoon',\n",
       "  'teaspoon',\n",
       "  'chopped',\n",
       "  'mustard',\n",
       "  'celery',\n",
       "  'tablespoons',\n",
       "  'teaspoonful',\n",
       "  'worcestershire'],\n",
       " 'infants': ['newborn',\n",
       "  'europeans',\n",
       "  'insulators',\n",
       "  'casein',\n",
       "  'iodinated',\n",
       "  'bandaging',\n",
       "  'splinting',\n",
       "  'disfavor',\n",
       "  'procaine'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'bockwurst',\n",
       "  'knackwurst',\n",
       "  'bologna',\n",
       "  'pepperoni',\n",
       "  'cervelat',\n",
       "  'conveniences',\n",
       "  'backyard',\n",
       "  'awnings'],\n",
       " 'schooling': ['needless',\n",
       "  'lessens',\n",
       "  'students',\n",
       "  'testifies',\n",
       "  'you',\n",
       "  'aid',\n",
       "  'provide',\n",
       "  'it',\n",
       "  'thirtyone'],\n",
       " 'tablespoon': ['teaspoon',\n",
       "  'worcestershire',\n",
       "  'teaspoons',\n",
       "  'sauerkraut',\n",
       "  'tablespoons',\n",
       "  'chive',\n",
       "  'ketchup',\n",
       "  'vinegar',\n",
       "  'horseradish'],\n",
       " 'teacher': ['children',\n",
       "  'country',\n",
       "  'little',\n",
       "  'group',\n",
       "  'present',\n",
       "  'man',\n",
       "  'young',\n",
       "  'a',\n",
       "  'old']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','mettwurst','teacher','cup','schooling','infants']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrBjI8skwbPc65rqtuT0/B",
   "collapsed_sections": [],
   "name": "Word2Vec - skipGram with hierarchical softmax and subsampling.ipynb",
   "provenance": [
    {
     "file_id": "1NTXl7J7usYrZZYSjpOiCdoKlD3K7fUXY",
     "timestamp": 1639934786740
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
