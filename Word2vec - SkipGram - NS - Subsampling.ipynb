{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13180,
     "status": "ok",
     "timestamp": 1639951148112,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "TKbGaf5uYpaD",
    "outputId": "4ee81433-11c6-419b-f731-437684a399da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "(100002346,)\n",
      "Word Count is: 12125\n",
      "Word Count Sum is 49420\n",
      "Sentence Count is: 4505\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "fulton county grand jury said an investigation atlantas recent primary election produced evidence that irregularities took place\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "jury further said termend presentments executive committee overall charge of election deserves praise thanks atlanta manner election conducted\n",
      "positive:\n",
      "[(1, [12125, 12125, 2, 3], [48, 76, 8330, 2398, 1037, 2395, 1037, 8859]), (2, [12125, 1, 3, 4], [1209, 10320, 7585, 3472, 1923, 5018, 23, 1406]), (3, [1, 2, 4, 5], [2475, 9984, 913, 2298, 691, 627, 6310, 338]), (4, [2, 3, 5, 7], [1257, 2635, 52, 3969, 7760, 221, 6232, 3990]), (5, [3, 4, 7, 8], [381, 155, 678, 75, 1847, 2529, 258, 294]), (7, [4, 5, 8, 10], [11847, 132, 830, 10268, 6883, 1571, 381, 4702]), (8, [5, 7, 10, 11], [9003, 11618, 7066, 5116, 8855, 63, 4243, 8113]), (10, [7, 8, 11, 12], [1682, 8950, 0, 8339, 462, 691, 362, 585]), (11, [8, 10, 12, 13], [3763, 381, 6548, 5890, 7852, 4093, 5898, 5537]), (12, [10, 11, 13, 14], [63, 164, 233, 7468, 4017, 4540, 261, 7669])]\n",
      "[('fulton', ['county', 'grand'], ['court', 'did', 'onetime', 'luncheon', 'requirement', 'expert', 'requirement', 'disposed']), ('county', ['fulton', 'grand', 'jury'], ['remainder', 'ideally', 'entomologist', 'receptive', 'different', 'windy', 'in', 'much']), ('grand', ['fulton', 'county', 'jury', 'said'], ['movement', 'nymphomaniacs', 'agency', 'frequently', 'i', 'aside', 'presbyterian', 'mrs']), ('jury', ['county', 'grand', 'said', 'an'], ['served', 'intentions', 'to', 'zimmerman', 'sholom', 'its', 'snake', 'christ']), ('said', ['grand', 'jury', 'an', 'investigation'], ['he', 'funds', 'board', 'it', 'mr', 'mitchell', 'new', 'work']), ('an', ['jury', 'said', 'investigation', 'atlantas'], ['lowliest', 'is', 'bill', 'colors', 'corresponding', 'following', 'he', 'exceptionally']), ('investigation', ['said', 'an', 'atlantas', 'recent'], ['starring', 'lumumba', 'refueling', 'balanced', 'deduction', 'a', 'armys', 'farmers']), ('atlantas', ['an', 'investigation', 'recent', 'primary'], ['ill', 'luncheons', 'the', 'closely', 'before', 'i', 'than', 'against']), ('recent', ['investigation', 'atlantas', 'primary', 'election'], ['firemen', 'he', 'meyer', 'exec', 'blues', 'hyde', 'favorite', 'inscription']), ('primary', ['atlantas', 'recent', 'election', 'produced'], ['a', 'at', 'into', 'ne', 'england', 'injury', 'when', 'handicapped'])]\n",
      "10\n",
      "positive:\n",
      "[(13, [11, 12, 14, 16], [137, 1975, 865, 4967, 10353, 7632, 7429, 3681]), (14, [12, 13, 16, 17], [8181, 4330, 92, 2809, 258, 1281, 3711, 2366]), (16, [13, 14, 17, 19], [381, 1208, 4916, 10077, 680, 2924, 198, 461]), (17, [14, 16, 19, 20], [261, 7050, 4481, 4698, 753, 8385, 4510, 3343]), (19, [16, 17, 20, 21], [1682, 159, 6659, 1287, 1369, 6291, 4230, 7066]), (20, [17, 19, 21, 12125], [879, 15, 1028, 159, 2713, 5957, 10263, 1719]), (21, [19, 20, 12125, 12125], [1847, 1239, 3900, 606, 488, 66, 842, 1786]), (4, [12125, 12125, 22, 5], [74, 159, 9691, 63, 7362, 5578, 5645, 186]), (22, [12125, 4, 5, 24], [5817, 942, 6221, 1094, 3069, 9342, 6503, 254]), (5, [4, 22, 24, 25], [2558, 3186, 740, 9848, 1785, 1833, 9134, 10589])]\n",
      "[('election', ['recent', 'primary', 'produced', 'evidence'], ['as', 'international', 'you', 'carl', 'china', 'flames', 'statutory', 'corporation']), ('produced', ['primary', 'election', 'evidence', 'that'], ['reichenberg', 'pitcher', 'these', 'calling', 'new', 'yesterday', 'injured', 'placed']), ('evidence', ['election', 'produced', 'that', 'irregularities'], ['he', 'subject', 'mad', 'aft', 'give', 'declined', 'might', 'says']), ('that', ['produced', 'evidence', 'irregularities', 'took'], ['when', 'missile', 'francisco', 'star', 'feared', 'eppler', 'hip', 'decade']), ('irregularities', ['evidence', 'that', 'took', 'place'], ['ill', 'so', 'sweet', 'york', 'knew', 'munger', 'foot', 'refueling']), ('took', ['that', 'irregularities', 'place'], ['denied', 'no', 'louis', 'so', 'sheeran', 'jana', 'givers', 'hour']), ('place', ['irregularities', 'took'], ['mr', 'university', 'willamette', 'expense', 'relations', 'such', 'now', 'ago']), ('jury', ['further', 'said'], ['this', 'so', 'fazio', 'a', 'cubans', 'heights', 'grassgreen', 'all']), ('further', ['jury', 'said', 'termend'], ['giacomo', 'get', 'influx', 'kind', 'dont', 'violin', 'col', 'servants']), ('said', ['jury', 'further', 'termend', 'presentments'], ['beating', 'search', 'death', 'capitalist', 'weeks', 'united', 'sculptured', 'disrepute'])]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self, sentences, sample):\n",
    "        self.norm_sentences = []\n",
    "        self.counter = 0\n",
    "        self.sample = sample\n",
    "        self.wordId_frequency_dict = dict()\n",
    "        self.word_count = 0  #  Number of words (repeated words only count as 1)\n",
    "        self.word_count_sum = 0  # Total number of words (the number of repeated words also accumulates)\n",
    "        self.sentence_count = 0  # Number of sentences\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict(sentences)  # Initialize the dictionary\n",
    "        self.subsampling()\n",
    "        self.sample_table = []\n",
    "        self._init_sample_table()\n",
    "        self.word_pairs_queue = deque()\n",
    "\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "\n",
    "    def special_match(strg, search=re.compile(r'[^a-z0-9.]').search):\n",
    "      return not bool(search(strg))\n",
    "\n",
    "\n",
    "    def subsampling(self):\n",
    "        \n",
    "        if self.sample > 0:\n",
    "            self.word_count_sum = 0\n",
    "            self.sentence_count = 0\n",
    "\n",
    "            frequency = np.array(list(self.wordId_frequency_dict.values()))\n",
    "            z = frequency / np.sum(frequency)\n",
    "            p = (np.sqrt(z / self.sample) + 1) * (self.sample / z)\n",
    "\n",
    "            new_norm_sentences = []\n",
    "            for word_list in self.norm_sentences:\n",
    "              word_list = [word for word in word_list if p[self.word2id_dict[word]] > random.random()]\n",
    "              if len(word_list) >= 2:\n",
    "                self.sentence_count += 1\n",
    "                self.word_count_sum += len(word_list)\n",
    "                new_norm_sentences.append(word_list)\n",
    "\n",
    "            self.norm_sentences = new_norm_sentences\n",
    "\n",
    "    def normalize(self, word_list):\n",
    "      sentence = \" \".join(word for word in word_list)\n",
    "      sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "      sentence = sentence.lower()\n",
    "      sentence = re.sub(' +', ' ', sentence)\n",
    "      sentence = sentence.strip()\n",
    "      norm_word_list = sentence.split(' ')\n",
    "      if self.sample <= 0:\n",
    "          stop_words = nltk.corpus.stopwords.words('english')\n",
    "          norm_word_list_with_out_stop_words = [word for word in norm_word_list if word not in stop_words]\n",
    "          norm_word_list = norm_word_list_with_out_stop_words\n",
    "\n",
    "      return norm_word_list\n",
    "      \n",
    "\n",
    "    def _init_dict(self, sentences):\n",
    "        word_freq = dict()\n",
    "        self.word_count_sum = 0\n",
    "        for word_list in sentences:\n",
    "            word_list = self.normalize(word_list)\n",
    "            if(len(word_list) < 2):\n",
    "                continue\n",
    "            self.word_count_sum += len(word_list)\n",
    "            self.sentence_count += 1\n",
    "            for word in word_list:\n",
    "                try:\n",
    "                    word_freq[word] += 1\n",
    "                except:\n",
    "                    word_freq[word] = 1\n",
    "            self.norm_sentences.append(word_list)\n",
    "\n",
    "        word_id = 0\n",
    "        # Initialize word2id_dict, id2word_dict, wordId_frequency_dict dictionary\n",
    "        for per_word, per_count in word_freq.items():\n",
    "            self.id2word_dict[word_id] = per_word\n",
    "            self.word2id_dict[per_word] = word_id\n",
    "            self.wordId_frequency_dict[word_id] = per_count\n",
    "            word_id += 1\n",
    "        self.word_count = len(self.word2id_dict)\n",
    "\n",
    "    def _init_sample_table(self):\n",
    "        sample_table_size = 1e8\n",
    "        frequency = np.array(list(self.wordId_frequency_dict.values())) ** 0.75\n",
    "        frequency_sum = np.sum(frequency)  # Total word frequency of all words\n",
    "        ratio_array = frequency / frequency_sum \n",
    "        word_count_list = np.round(ratio_array * sample_table_size)\n",
    "        for word_index, word_freq in enumerate(word_count_list):\n",
    "            self.sample_table += [word_index] * int(word_freq)  # it generates a list, the content is the id of each word, each id in the list is repeated multiple times, the number of repetitions is the word frequency\n",
    "        self.sample_table = np.array(self.sample_table)\n",
    "        print(self.sample_table.shape)\n",
    "\n",
    "    def generate_positive_pairs(self, window_size, neg_count):\n",
    "        self.counter += 1\n",
    "        if not self.norm_sentences[20*(self.counter-1):20*self.counter]:\n",
    "            self.counter = 1\n",
    "            self.word_pairs_queue.clear()\n",
    "        sub_wids = [[self.word2id_dict[word] for word in word_list] for word_list in self.norm_sentences[20*(self.counter-1):20*self.counter]]\n",
    "\n",
    "        # Find the positive sampling pair (w,v) and add it to the positive sampling queue\n",
    "        for words in sub_wids:\n",
    "          sentence_length = len(words)\n",
    "          for index, center_word in enumerate(words):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            positive_words = []\n",
    "            for index_2 in range(start,end):\n",
    "              if 0 <= index_2 < sentence_length and index_2 != index:\n",
    "                positive_words.append(words[index_2])\n",
    "              elif index_2 < 0 or index_2 >= sentence_length:\n",
    "                positive_words.append(self.word_count)\n",
    "            \n",
    "            negative_words = np.random.choice(self.sample_table, size=neg_count).tolist()\n",
    "\n",
    "            self.word_pairs_queue.append((center_word, positive_words, negative_words))\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size, neg_count):\n",
    "\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "          self.generate_positive_pairs(window_size, neg_count)              \n",
    "              \n",
    "        result_pairs = []  # Returns a positive sample pair of mini-batch size\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    def evaluate_pairs_count(self):\n",
    "        return self.word_count_sum\n",
    "\n",
    "\n",
    "def test():\n",
    "    sentences = brown.sents(categories=['news'])\n",
    "    test_data = InputData(sentences,0.0001)\n",
    "    print(\" \".join(word for word in sentences[0]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[0]))\n",
    "    print(\" \".join(word for word in sentences[1]))\n",
    "    print(\" \".join(word for word in test_data.norm_sentences[1]))\n",
    "    # test_data.evaluate_pairs_count()\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], [test_data.id2word_dict[i] for i in pair[1] if i != test_data.word_count], [test_data.id2word_dict[i] for i in pair[2] if i != test_data.word_count]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "    pos_pairs = test_data.get_batch_pairs(10, 2, 8)\n",
    "    print('positive:')\n",
    "    print(pos_pairs)\n",
    "    pos_word_pairs = []\n",
    "    for pair in pos_pairs:\n",
    "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], [test_data.id2word_dict[i] for i in pair[1] if i != test_data.word_count], [test_data.id2word_dict[i] for i in pair[2] if i != test_data.word_count]))\n",
    "    print(pos_word_pairs)\n",
    "    print(len(pos_pairs))\n",
    "\n",
    "    # neg_pair = test_data.get_negative_sampling(pos_pairs, 3)\n",
    "    # print('negative:')\n",
    "    # print(neg_pair)\n",
    "    # neg_word_pair = []\n",
    "    #for pair in neg_pair:\n",
    "    #    neg_word_pair.append(\n",
    "    #        (test_data.id2word_dict[pair[0]], test_data.id2word_dict[pair[1]], test_data.id2word_dict[pair[2]]))\n",
    "    #print(neg_word_pair)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1639951193052,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "UunHC6-rZBpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.w_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size + 1, emb_dimension, sparse=True)\n",
    "        self._init_emb()\n",
    "\n",
    "    def _init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange) # work\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0) # work\n",
    "\n",
    "    def forward(self, pos_w, pos_v, neg_v):\n",
    "        emb_w = self.w_embeddings(torch.LongTensor(pos_w))\n",
    "        emb_v = self.v_embeddings(torch.LongTensor(pos_v))\n",
    "        neg_emb_v = self.v_embeddings(torch.LongTensor(neg_v))\n",
    "\n",
    "        score = torch.mul(emb_w.unsqueeze(1), emb_v)\n",
    "        score = torch.sum(score, dim=2).squeeze()\n",
    "        score = F.logsigmoid(score)\n",
    "        score = torch.sum(score, dim=1).squeeze()\n",
    "\n",
    "        neg_score = torch.mul(emb_w.unsqueeze(1), neg_emb_v)\n",
    "        neg_score = torch.sum(neg_score, dim=2).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        neg_score = torch.sum(neg_score, dim=1).squeeze()\n",
    "\n",
    "        # L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]\n",
    "        final_score = score + neg_score\n",
    "        loss = -1 * torch.sum(final_score)\n",
    "        return loss\n",
    "\n",
    "    def distance_matrix(self, word_count):\n",
    "        embedding = self.w_embeddings.weight.data.numpy()[:word_count]\n",
    "        distance_matrix = euclidean_distances(embedding)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = SkipGramModel(100, 10)\n",
    "    id2word = dict()\n",
    "    for i in range(100):\n",
    "        id2word[i] = str(i)\n",
    "    pos_w = [0, 2]\n",
    "    pos_v = [[9,10],[10,12]]\n",
    "    neg_v = [[23, 42, 74, 32], [32, 24, 62, 53]]\n",
    "    model.forward(pos_w, pos_v, neg_v)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1639951202667,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "z6pcTUByZP0D"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyper parameters\n",
    "WINDOW_SIZE = 4 \n",
    "BATCH_SIZE = 1000  # mini-batch\n",
    "EMB_DIMENSION = 100  # embedding dimension\n",
    "LR = 0.01 # Learning rate\n",
    "NEG_COUNT = 12\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, sentences, sample):\n",
    "        self.data = InputData(sentences, sample)\n",
    "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"SkipGram Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count()\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        for epoch in range(1,51):\n",
    "            mean_loss = 0\n",
    "            process_bar = tqdm(range(int(batch_count)))\n",
    "            for i in process_bar:\n",
    "                pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE, NEG_COUNT)\n",
    "                pos_w = [int(pair[0]) for pair in pos_pairs]\n",
    "                pos_v = [pair[1] for pair in pos_pairs]\n",
    "                neg_v = [pair[2] for pair in pos_pairs]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_w, pos_v, neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                mean_loss += loss\n",
    "\n",
    "            print(\"epoch:\",epoch,\"loss:\",mean_loss/int(batch_count))\n",
    "\n",
    "\n",
    "    def get_distance_matrix(self):\n",
    "        distance_matrix = self.model.distance_matrix(self.data.word_count)\n",
    "        return distance_matrix\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     w2v = Word2Vec(input_file_name='./data.txt', output_file_name=\"word_embedding.txt\")\n",
    "#     w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12728,
     "status": "ok",
     "timestamp": 1639951221600,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "qBk19rOeBHAs",
    "outputId": "34821bf9-dc57-4dd7-c31a-c49aeaaa59eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100002426,)\n",
      "Word Count is: 24758\n",
      "Word Count Sum is 196690\n",
      "Sentence Count is: 17269\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "SAMPLE = 0.0002 # use subsampling\n",
    "w2v = Word2Vec(sentences, SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15935,
     "status": "ok",
     "timestamp": 1639951239773,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "W1UqSn24m9mq",
    "outputId": "bf1ac703-cedf-4fbd-c8df-934d3abbadb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999555,)\n",
      "Word Count is: 24616\n",
      "Word Count Sum is 170964\n",
      "Sentence Count is: 17106\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories=['news','reviews','government','hobbies','romance'])\n",
    "SAMPLE = 0 # eliminate stop words\n",
    "w2v2 = Word2Vec(sentences, SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 823473,
     "status": "ok",
     "timestamp": 1639952073434,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "jIkOnp9GETIS",
    "outputId": "3e4495a3-0a25-4067-9e0b-370c2c0b814e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Training......\n",
      "pairs_count 196690\n",
      "batch_count 196.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(12915.3281, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(12333.5928, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(12195.7715, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:17<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(12093.3320, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(11996.3818, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(11900.4346, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(11795.2832, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(11690.0762, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(11584.6904, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(11478.4404, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(11365.2578, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(11253.7959, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(11138.0596, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(11017.2705, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(10892.2295, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(10764.9629, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(10628.2715, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(10493.9482, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(10351.6270, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:19<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(10211.1025, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 loss: tensor(10067.9268, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 loss: tensor(9919.2617, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 loss: tensor(9769.2764, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:17<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 loss: tensor(9623.7803, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:18<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 loss: tensor(9482.3770, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 loss: tensor(9330.1553, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 loss: tensor(9189.7949, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 loss: tensor(9049.6465, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: tensor(8911.3760, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:18<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: tensor(8780.0195, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 loss: tensor(8646.9697, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss: tensor(8521.3115, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 loss: tensor(8402.7354, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:17<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 loss: tensor(8281.8301, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 loss: tensor(8170.1294, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 loss: tensor(8053.9336, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 loss: tensor(7952.1309, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 loss: tensor(7857.8984, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 loss: tensor(7744.8110, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: tensor(7654.2358, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 loss: tensor(7561.1694, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 loss: tensor(7467.4688, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 loss: tensor(7390.3311, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 loss: tensor(7303.3486, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 loss: tensor(7224.8062, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 loss: tensor(7149.9609, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 loss: tensor(7080.8311, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:17<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 loss: tensor(7000.9526, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:16<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss: tensor(6939.3960, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:15<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(6866.7231, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752806,
     "status": "ok",
     "timestamp": 1639952928447,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "2Mga4yWXuF4C",
    "outputId": "cfd83e85-1819-4159-b71e-dac63798ae67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Training......\n",
      "pairs_count 170964\n",
      "batch_count 170.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(12976.0547, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: tensor(12376.5703, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: tensor(12164.6338, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: tensor(12000.3662, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: tensor(11865.2188, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: tensor(11750.2432, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: tensor(11645.7363, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: tensor(11534.1689, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: tensor(11415.5732, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: tensor(11294.0088, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: tensor(11170.5078, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: tensor(11048.5518, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: tensor(10925.1367, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: tensor(10798.7754, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: tensor(10672.6113, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss: tensor(10546.7412, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 loss: tensor(10410.6113, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 loss: tensor(10280.6338, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss: tensor(10143.3203, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: tensor(10004.5078, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 loss: tensor(9863.5117, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 loss: tensor(9714.2783, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 loss: tensor(9569.9229, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 loss: tensor(9420.4600, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 loss: tensor(9265.7959, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 loss: tensor(9108.7617, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 loss: tensor(8949.3564, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 loss: tensor(8792.6084, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:13<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: tensor(8633.1113, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: tensor(8479.1016, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 loss: tensor(8314.6348, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss: tensor(8150.0991, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:16<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 loss: tensor(8006.3550, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:16<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 loss: tensor(7852.0942, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 loss: tensor(7700.6304, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 loss: tensor(7554.8027, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 loss: tensor(7417.6587, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 loss: tensor(7270.3433, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 loss: tensor(7136.3037, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: tensor(7002.5840, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 loss: tensor(6876.0957, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 loss: tensor(6755.6274, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 loss: tensor(6641.8213, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 loss: tensor(6526.2568, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 loss: tensor(6412.3862, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 loss: tensor(6309.8862, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 loss: tensor(6211.3330, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 loss: tensor(6108.8188, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:14<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss: tensor(6022.3667, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:15<00:00, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(5930.6221, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 22863,
     "status": "ok",
     "timestamp": 1639952971083,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "GYx4RpJkIzU_"
   },
   "outputs": [],
   "source": [
    "distance_matrix = w2v.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 17582,
     "status": "ok",
     "timestamp": 1639953011560,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "WE0laG6KudIG"
   },
   "outputs": [],
   "source": [
    "distance_matrix2 = w2v2.get_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1639953017684,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "TOK9m2aBI79I",
    "outputId": "2afa4a5d-2bae-4799-ce20-f21f17605816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ballot': ['disturbance',\n",
       "  'nominating',\n",
       "  'referendum',\n",
       "  'nonpartisan',\n",
       "  'warrant',\n",
       "  'cursing',\n",
       "  'pardon',\n",
       "  'curiae',\n",
       "  'septemberoctober'],\n",
       " 'car': ['gloriana',\n",
       "  'jenks',\n",
       "  'collided',\n",
       "  'mystical',\n",
       "  'insured',\n",
       "  'halfdozen',\n",
       "  'ida',\n",
       "  'thermostat',\n",
       "  'commandeering'],\n",
       " 'election': ['hearings',\n",
       "  'justices',\n",
       "  'decertify',\n",
       "  'coolest',\n",
       "  'closeness',\n",
       "  'advisement',\n",
       "  'indicating',\n",
       "  'subpenas',\n",
       "  'mandate'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'bockwurst',\n",
       "  'cervelat',\n",
       "  'haec',\n",
       "  'stabat',\n",
       "  'pergolesis',\n",
       "  'knackwurst',\n",
       "  'tallchief',\n",
       "  'flavus'],\n",
       " 'player': ['birdied',\n",
       "  'pensacola',\n",
       "  'strokes',\n",
       "  'tee',\n",
       "  'paired',\n",
       "  'mano',\n",
       "  'closest',\n",
       "  'winnings',\n",
       "  'fourwood'],\n",
       " 'republican': ['resentment',\n",
       "  'nominee',\n",
       "  'sharkey',\n",
       "  'bookwalter',\n",
       "  'bronx',\n",
       "  'ptc',\n",
       "  'caucus',\n",
       "  'laughlin',\n",
       "  'pardon'],\n",
       " 'sauce': ['tablespoon',\n",
       "  'teaspoon',\n",
       "  'mustard',\n",
       "  'sweetsour',\n",
       "  'sauerkraut',\n",
       "  'teaspoons',\n",
       "  'dough',\n",
       "  'worcestershire',\n",
       "  'tablespoons'],\n",
       " 'tablespoon': ['teaspoons',\n",
       "  'worcestershire',\n",
       "  'teaspoon',\n",
       "  'chunks',\n",
       "  'vinegar',\n",
       "  'celery',\n",
       "  'sauerkraut',\n",
       "  'paprika',\n",
       "  'chopped'],\n",
       " 'university': ['desiring',\n",
       "  'faculties',\n",
       "  'northwestern',\n",
       "  'graduate',\n",
       "  'emory',\n",
       "  'cambridge',\n",
       "  'wellesley',\n",
       "  'instructor',\n",
       "  'rebuilds']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v.data.id2word_dict[idx] for idx in distance_matrix[w2v.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','election','sauce', 'ballot','mettwurst','car','player','university','republican']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1639953039497,
     "user": {
      "displayName": "moghis fereidouni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVlp_EUmsYa2wET4sH8v8ttME5Wjz5knmZivaz=s64",
      "userId": "04154357719458086941"
     },
     "user_tz": -210
    },
    "id": "FYj-9vngufj7",
    "outputId": "e8b6b462-c675-496d-9e7e-916750b97f3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ballot': ['anonymous',\n",
       "  'referendum',\n",
       "  'nominating',\n",
       "  'nonpartisan',\n",
       "  'pertained',\n",
       "  'subdue',\n",
       "  'wrongful',\n",
       "  'jurytampering',\n",
       "  'miscount'],\n",
       " 'car': ['rental',\n",
       "  'gloriana',\n",
       "  'vernava',\n",
       "  'touring',\n",
       "  'chauffeurdriven',\n",
       "  'pezza',\n",
       "  'rear',\n",
       "  'driving',\n",
       "  'renting'],\n",
       " 'election': ['voters',\n",
       "  'anonymous',\n",
       "  'coolest',\n",
       "  'justices',\n",
       "  'arrests',\n",
       "  'crossroads',\n",
       "  'subpenas',\n",
       "  'republicanism',\n",
       "  'confession'],\n",
       " 'mettwurst': ['bratwurst',\n",
       "  'bockwurst',\n",
       "  'knackwurst',\n",
       "  'oleg',\n",
       "  'bologna',\n",
       "  'ambrose',\n",
       "  'bierce',\n",
       "  'neuritis',\n",
       "  'pergolesis'],\n",
       " 'player': ['strokes',\n",
       "  'augusta',\n",
       "  'threeround',\n",
       "  'tee',\n",
       "  'birdied',\n",
       "  'mano',\n",
       "  'winnings',\n",
       "  'palmers',\n",
       "  'tournament'],\n",
       " 'republican': ['nominee',\n",
       "  'carcass',\n",
       "  'humphrey',\n",
       "  'conservatives',\n",
       "  'dirksen',\n",
       "  'hubert',\n",
       "  'mississippis',\n",
       "  'liberals',\n",
       "  'sheeran'],\n",
       " 'sauce': ['tablespoon',\n",
       "  'mustard',\n",
       "  'tablespoons',\n",
       "  'teaspoon',\n",
       "  'chili',\n",
       "  'worcestershire',\n",
       "  'sauerkraut',\n",
       "  'sweetsour',\n",
       "  'teaspoons'],\n",
       " 'tablespoon': ['teaspoon',\n",
       "  'worcestershire',\n",
       "  'teaspoons',\n",
       "  'tablespoons',\n",
       "  'sauerkraut',\n",
       "  'vinegar',\n",
       "  'chunks',\n",
       "  'chili',\n",
       "  'toner'],\n",
       " 'university': ['emory',\n",
       "  'creed',\n",
       "  'copenhagen',\n",
       "  'astronomy',\n",
       "  'harvard',\n",
       "  'participates',\n",
       "  'physics',\n",
       "  'paramount',\n",
       "  'faculties']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [w2v2.data.id2word_dict[idx] for idx in distance_matrix2[w2v2.data.word2id_dict[search_term]].argsort()[1:10]] \n",
    "                   for search_term in ['tablespoon','election','sauce', 'ballot','mettwurst','car','player','university','republican']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMg6T4I8dqSiPMcjmvDJYXW",
   "collapsed_sections": [],
   "name": "Word2Vec - skipGram with negative sampling and subsampling.ipynb",
   "provenance": [
    {
     "file_id": "1caWX3u6zHAL8KviSlakIKe9U54-Bqe54",
     "timestamp": 1639833373580
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
